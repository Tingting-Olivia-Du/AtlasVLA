# åˆ†å¸ƒå¼è®­ç»ƒDebugè§£å†³æ–¹æ¡ˆ

## ğŸ” é—®é¢˜åˆ†æ

### é”™è¯¯ç—‡çŠ¶
```
torch.distributed.elastic.multiprocessing.errors.ChildFailedError
Root Cause: rank 0 (local_rank: 0), exitcode: 1
```

è®­ç»ƒåœ¨`Initializing trainer...`é˜¶æ®µå´©æºƒï¼Œæ—¥å¿—æ˜¾ç¤ºæ ¼å¼é”™è¯¯ï¼š
```
Base LR: 1e-4, Effective LR (scaled by 8): 1e-41e-41e-41e-41e-41e-41e-4
```

**è¿™ä¸ªé”™è¯¯çœ‹èµ·æ¥åƒå¤šè¿›ç¨‹æ—¥å¿—ç«äº‰ï¼Œä½†å®é™…æ˜¯YAMLé…ç½®ç±»å‹é”™è¯¯ï¼**

### æ ¹æœ¬åŸå›  âš ï¸ CRITICAL BUG

#### 1. **YAMLé…ç½®ç±»å‹é”™è¯¯** ğŸ”¥ CRITICAL
**ä½ç½®**: `atlas/configs/train_config.yaml` + `atlas/train.py`

**é—®é¢˜**: YAMLå°†ç§‘å­¦è®¡æ•°æ³•`1e-4`è§£æä¸º**å­—ç¬¦ä¸²**è€Œä¸æ˜¯æµ®ç‚¹æ•°ï¼

```yaml
# train_config.yaml (é”™è¯¯)
learning_rate: 1e-4  # è¢«è§£æä¸ºå­—ç¬¦ä¸² "1e-4"
```

```python
# train.py (é”™è¯¯)
base_lr = training_config["learning_rate"]  # base_lr = "1e-4" (å­—ç¬¦ä¸²!)
effective_lr = base_lr * world_size  # "1e-4" * 8 = "1e-41e-41e-41e-41e-41e-41e-41e-4"
```

**å½±å“**: 
- å­—ç¬¦ä¸²ä¹˜æ³•å¯¼è‡´é‡å¤å­—ç¬¦ä¸²ï¼Œä¸æ˜¯æ•°å€¼è®¡ç®—
- Traineråˆå§‹åŒ–æ—¶ä¼ å…¥å­—ç¬¦ä¸²å¯¼è‡´å´©æºƒ
- æ—¥å¿—æ˜¾ç¤ºæ··ä¹±çš„æ ¼å¼ï¼š`1e-41e-41e-41e-41e-41e-41e-41e-4`

**æ ¹æœ¬åŸå› **: YAMLçš„ç§‘å­¦è®¡æ•°æ³•å¿…é¡»å¸¦å°æ•°ç‚¹ï¼ˆå¦‚`1.0e-4`ï¼‰æˆ–ä½¿ç”¨åè¿›åˆ¶ï¼ˆå¦‚`0.0001`ï¼‰ï¼Œå¦åˆ™ä¼šè¢«å½“ä½œå­—ç¬¦ä¸²

#### 2. **HuggingFaceæ•°æ®é›†å¹¶å‘åŠ è½½** âš ï¸
**é—®é¢˜**: æ‰€æœ‰8ä¸ªè¿›ç¨‹åŒæ—¶åŠ è½½æ•°æ®é›†ï¼Œå¯¼è‡´ï¼š
- å¤§é‡å¹¶å‘HTTPè¯·æ±‚ï¼ˆæ¯ä¸ªepisodeçš„HEADè¯·æ±‚ Ã— 8ä¸ªè¿›ç¨‹ï¼‰
- ç½‘ç»œæ‹¥å¡å’Œè¶…æ—¶
- å¯èƒ½çš„ç¼“å­˜æ–‡ä»¶ç«äº‰

**å½±å“**:
- æ•°æ®åŠ è½½ææ…¢ï¼ˆæ—¥å¿—æ˜¾ç¤ºèŠ±è´¹17åˆ†é’Ÿä»åœ¨åŠ è½½å…ƒæ•°æ®ï¼‰
- å¯èƒ½è§¦å‘è¿›ç¨‹è¶…æ—¶
- æµªè´¹ç½‘ç»œå¸¦å®½

#### 3. **æ•°æ®é›†éæµå¼æ¨¡å¼**
**é…ç½®**: `streaming: false` åœ¨ `train_config.yaml`
- ä¼šå°è¯•ä¸‹è½½æ•´ä¸ªLIBEROæ•°æ®é›†ï¼ˆæ•°ç™¾GBï¼‰
- å¤§é‡parquetæ–‡ä»¶éœ€è¦é€ä¸ªéªŒè¯

## âœ… ä¿®å¤æ–¹æ¡ˆ

### ä¿®å¤1: å¼ºåˆ¶ç±»å‹è½¬æ¢ ğŸ”¥ CRITICAL FIX
**æ–‡ä»¶**: `atlas/train.py`

```python
# ä¿®å¤åçš„ä»£ç 
base_lr = float(training_config["learning_rate"])  # å¼ºåˆ¶è½¬æ¢ä¸ºfloat
if is_distributed:
    effective_lr = base_lr * world_size
else:
    effective_lr = base_lr

if rank == 0:
    if is_distributed:
        logging.info(f"  Base LR: {base_lr}, Effective LR (scaled by {world_size}): {effective_lr}")
    else:
        logging.info(f"  Learning rate: {effective_lr}")
```

**é…ç½®æ–‡ä»¶ä¿®å¤**: `atlas/configs/train_config.yaml`

```yaml
# æ–¹æ¡ˆA: ä½¿ç”¨åè¿›åˆ¶æ ¼å¼ï¼ˆæ¨èï¼‰
learning_rate: 0.0001

# æ–¹æ¡ˆB: ä½¿ç”¨å¸¦å°æ•°ç‚¹çš„ç§‘å­¦è®¡æ•°æ³•
learning_rate: 1.0e-4
```

**æ•ˆæœ**: 
- ç¡®ä¿learning_rateå§‹ç»ˆæ˜¯æµ®ç‚¹æ•°
- æ•°å€¼è®¡ç®—æ­£ç¡®ï¼š`0.0001 * 8 = 0.0008`
- æ—¥å¿—æ ¼å¼æ­£ç¡®ï¼š`Base LR: 0.0001, Effective LR (scaled by 8): 0.0008`

### ä¿®å¤2: åˆ†å¸ƒå¼æ•°æ®åŠ è½½åŒæ­¥ (æ€§èƒ½ä¼˜åŒ–)
**æ–‡ä»¶**: `atlas/train.py`

```python
# åˆ†å¸ƒå¼è®­ç»ƒï¼šrank 0å…ˆåŠ è½½å¹¶ç¼“å­˜ï¼Œå…¶ä»–ranksç­‰å¾…åå†åŠ è½½
if is_distributed:
    if rank == 0:
        logging.info("  Rank 0: Loading dataset (will be cached for other ranks)...")
        train_dataset = LIBEROHFDataset(...)
        logging.info("  Rank 0: Dataset loaded, waiting for other ranks...")
    
    # åŒæ­¥ï¼šç­‰å¾…rank 0å®ŒæˆåŠ è½½/ç¼“å­˜
    dist.barrier()
    
    # å…¶ä»–ranksä»ç¼“å­˜åŠ è½½
    if rank != 0:
        train_dataset = LIBEROHFDataset(...)
else:
    # å•GPUï¼šæ­£å¸¸åŠ è½½
    train_dataset = LIBEROHFDataset(...)
```

**æ•ˆæœ**:
- åªæœ‰rank 0è¿›è¡Œåˆå§‹ä¸‹è½½/ç¼“å­˜
- å…¶ä»–rankså¤ç”¨ç¼“å­˜ï¼Œé¿å…é‡å¤è¯·æ±‚
- å¤§å¹…å‡å°‘ç½‘ç»œè´Ÿè½½å’ŒåŠ è½½æ—¶é—´

### ä¿®å¤3: æ¨èä½¿ç”¨æµå¼æ¨¡å¼ï¼ˆå¯é€‰ï¼‰
**æ–‡ä»¶**: `atlas/configs/train_config.yaml`

```yaml
data:
  streaming: true  # æ”¹ä¸ºtrueï¼ŒæŒ‰éœ€åŠ è½½æ•°æ®
```

**ä¼˜ç‚¹**:
- ä¸éœ€è¦ä¸‹è½½æ•´ä¸ªæ•°æ®é›†
- èŠ‚çœç£ç›˜ç©ºé—´
- åŠ è½½é€Ÿåº¦æ›´å¿«

**ç¼ºç‚¹**:
- éœ€è¦ç¨³å®šç½‘ç»œè¿æ¥
- æ¯ä¸ªepochå¯èƒ½æœ‰ç½‘ç»œå»¶è¿Ÿ

## ğŸ§ª éªŒè¯æ­¥éª¤

### 1. æ£€æŸ¥ä¿®å¤æ˜¯å¦ç”Ÿæ•ˆ
```bash
# æ¸…é™¤æ—§æ—¥å¿—
rm -rf logs/*.log

# é‡æ–°è¿è¡Œè®­ç»ƒ
cd /workspace/02042026_tingting/AtlasVLA
bash atlas/scripts/train.sh
```

### 2. è§‚å¯Ÿæ—¥å¿—
- æ—¥å¿—åº”è¯¥æ¸…æ™°ï¼Œæ²¡æœ‰æ ¼å¼é”™è¯¯
- åº”è¯¥çœ‹åˆ° "Rank 0: Loading dataset..." å’Œ barrieråŒæ­¥ä¿¡æ¯
- æ•°æ®åŠ è½½åº”è¯¥æ›´å¿«ï¼ˆåªæœ‰ä¸€ä¸ªè¿›ç¨‹ä¸‹è½½ï¼‰

### 3. ç›‘æ§è¿›ç¨‹
```bash
# æŸ¥çœ‹GPUä½¿ç”¨æƒ…å†µ
watch -n 1 nvidia-smi

# æŸ¥çœ‹è¿›ç¨‹çŠ¶æ€
ps aux | grep python
```

## ğŸ“Š é¢„æœŸæ”¹è¿›

| æŒ‡æ ‡ | ä¿®å¤å‰ | ä¿®å¤å |
|------|--------|--------|
| Learning rate | å­—ç¬¦ä¸² "1e-4" | æµ®ç‚¹æ•° 0.0001 |
| æ—¥å¿—æ ¼å¼ | `1e-41e-41e-4...` | `0.0001` / `8e-04` |
| Traineråˆå§‹åŒ– | å´©æºƒ (ç±»å‹é”™è¯¯) | æˆåŠŸ |
| æ•°æ®åŠ è½½æ—¶é—´ | 17+ åˆ†é’Ÿï¼ˆæœªå®Œæˆï¼‰ | 2-5 åˆ†é’Ÿï¼ˆä½¿ç”¨barrierï¼‰ |
| ç½‘ç»œè¯·æ±‚ | 8xå¹¶å‘ | 1xä¸²è¡Œ |
| è¿›ç¨‹å´©æºƒ | rank 0 å´©æºƒ | ç¨³å®šè¿è¡Œ |

## ğŸ”§ å…¶ä»–ä¼˜åŒ–å»ºè®®

### 1. å¢åŠ åˆ†å¸ƒå¼è¶…æ—¶æ—¶é—´
å¦‚æœç½‘ç»œè¾ƒæ…¢ï¼Œå¯ä»¥å¢åŠ è¶…æ—¶ï¼š
```bash
export TORCH_DISTRIBUTED_TIMEOUT=3600  # 1å°æ—¶
```

### 2. è®¾ç½®HuggingFaceç¼“å­˜ç›®å½•
ä½¿ç”¨ä¸“ç”¨ç¼“å­˜ç›®å½•é¿å…æƒé™é—®é¢˜ï¼š
```yaml
data:
  hf_cache_dir: "/workspace/hf_cache"  # ä½¿ç”¨å›ºå®šç¼“å­˜ç›®å½•
```

### 3. ä½¿ç”¨æ›´å°çš„batch sizeï¼ˆå¦‚æœå†…å­˜ä¸è¶³ï¼‰
```yaml
data:
  batch_size: 4  # ä»8å‡å°‘åˆ°4
```

### 4. å¯ç”¨æ¢¯åº¦ç´¯ç§¯ï¼ˆæ¨¡æ‹Ÿæ›´å¤§batch sizeï¼‰
```yaml
training:
  gradient_accumulation_steps: 2  # æœ‰æ•ˆbatch size = 4 Ã— 8 GPUs Ã— 2 = 64
```

## ğŸ“ æ€»ç»“

ä¸»è¦é—®é¢˜æ˜¯**YAMLé…ç½®ç±»å‹é”™è¯¯** - ç§‘å­¦è®¡æ•°æ³•`1e-4`è¢«é”™è¯¯è§£æä¸ºå­—ç¬¦ä¸²ï¼Œå¯¼è‡´å­—ç¬¦ä¸²ä¹˜æ³•è€Œä¸æ˜¯æ•°å€¼è®¡ç®—ã€‚

### å…³é”®æ•™è®­
1. **YAMLæ•°å€¼æ ¼å¼**: ç§‘å­¦è®¡æ•°æ³•éœ€è¦å°æ•°ç‚¹ï¼ˆ`1.0e-4`ï¼‰æˆ–ä½¿ç”¨åè¿›åˆ¶ï¼ˆ`0.0001`ï¼‰
2. **ç±»å‹å®‰å…¨**: ä»é…ç½®è¯»å–æ•°å€¼æ—¶åº”è¯¥å¼ºåˆ¶ç±»å‹è½¬æ¢ `float()`
3. **é”™è¯¯è¡¨è±¡**: `1e-41e-41e-4...` ä¸æ˜¯å¤šè¿›ç¨‹æ—¥å¿—ç«äº‰ï¼Œæ˜¯å­—ç¬¦ä¸²ä¹˜æ³•ï¼

### ä¿®å¤æ€»ç»“
1. âœ… **Critical**: `float()`å¼ºåˆ¶ç±»å‹è½¬æ¢
2. âœ… **Config**: ä¿®æ”¹YAMLä¸º`0.0001`æ ¼å¼
3. âœ… **Performance**: æ·»åŠ `dist.barrier()`ä¼˜åŒ–æ•°æ®åŠ è½½

ä¿®å¤ååº”è¯¥èƒ½å¤Ÿç¨³å®šè¿›è¡Œåˆ†å¸ƒå¼è®­ç»ƒã€‚
