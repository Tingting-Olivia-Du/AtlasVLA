# VLA-VGGT å®ç°æ€»ç»“

## ğŸ“Œ é¡¹ç›®æ¦‚è¿°

æœ¬é¡¹ç›®å®ç°äº†ä¸€ä¸ªåŸºäº VGGT çš„ Vision-Language-Action (VLA) æ¨¡å‹ï¼Œç”¨äºæœºå™¨äººæ“ä½œä»»åŠ¡ã€‚

**å…³é”®æ”¹è¿›**:
1. âœ… æ”¯æŒä» HuggingFace åŠ è½½ `facebook/vggt`
2. âœ… æ”¯æŒ Qwen3-0.6B-Base è¯­è¨€æ¨¡å‹
3. âœ… çµæ´»çš„ vision encoder (ç›´æ¥ patch embedding æˆ–é¢„è®­ç»ƒ vision tower)
4. âœ… æ”¹è¿›çš„å¤šæ¨¡æ€èåˆç­–ç•¥
5. âœ… HuggingFace LIBERO æ•°æ®é›†é›†æˆ

---

## ğŸ“‚ é¡¹ç›®ç»“æ„

```
vggt_vla/
â”œâ”€â”€ configs/                      # é…ç½®æ–‡ä»¶
â”‚   â”œâ”€â”€ model_config.py          # æ¨¡å‹é…ç½®ç±»
â”‚   â”œâ”€â”€ train_simple.yaml        # ç®€å•é…ç½®
â”‚   â”œâ”€â”€ train_with_dinov2.yaml   # ä½¿ç”¨ DINOv2
â”‚   â””â”€â”€ train_full.yaml          # å®Œæ•´é…ç½®
â”‚
â”œâ”€â”€ models/                       # æ¨¡å‹å®ç°
â”‚   â”œâ”€â”€ vision_encoder.py        # è§†è§‰ç¼–ç å™¨ (æ”¯æŒå¤šç§æ–¹æ¡ˆ)
â”‚   â”œâ”€â”€ language_encoder.py      # è¯­è¨€ç¼–ç å™¨ (Qwen3-0.6B)
â”‚   â”œâ”€â”€ vggt_adapter.py          # VGGT é€‚é…å™¨ (HF + ç®€åŒ–ç‰ˆ)
â”‚   â”œâ”€â”€ vggt_backbone.py         # åŸå§‹ VGGT backbone
â”‚   â”œâ”€â”€ action_head.py           # åŠ¨ä½œé¢„æµ‹å¤´
â”‚   â”œâ”€â”€ vla_model.py             # å®Œæ•´ VLA æ¨¡å‹
â”‚   â””â”€â”€ components/              # VGGT ç»„ä»¶
â”‚       â”œâ”€â”€ vggt_layers.py       # VGGT å±‚
â”‚       â”œâ”€â”€ token_fusion.py      # Token èåˆ
â”‚       â””â”€â”€ graph_builder.py     # å›¾æ„å»º
â”‚
â”œâ”€â”€ data/                         # æ•°æ®åŠ è½½
â”‚   â”œâ”€â”€ libero_dataset.py        # LIBERO æœ¬åœ°æ•°æ®é›†
â”‚   â””â”€â”€ libero_hf_dataset.py     # LIBERO HuggingFace æ•°æ®é›†
â”‚
â”œâ”€â”€ training/                     # è®­ç»ƒç›¸å…³
â”‚   â”œâ”€â”€ trainer.py               # è®­ç»ƒå¾ªç¯
â”‚   â”œâ”€â”€ losses.py                # æŸå¤±å‡½æ•°
â”‚   â””â”€â”€ metrics.py               # è¯„ä¼°æŒ‡æ ‡
â”‚
â”œâ”€â”€ scripts/                      # è„šæœ¬
â”‚   â”œâ”€â”€ train_vla.py             # ä¸»è®­ç»ƒè„šæœ¬
â”‚   â”œâ”€â”€ test_model.py            # æ¨¡å‹æµ‹è¯•
â”‚   â”œâ”€â”€ eval.py                  # è¯„ä¼°è„šæœ¬
â”‚   â””â”€â”€ quick_start.sh           # å¿«é€Ÿå¯åŠ¨
â”‚
â”œâ”€â”€ README.md                     # ç”¨æˆ·æ–‡æ¡£
â”œâ”€â”€ ARCHITECTURE_ANALYSIS.md      # æ¶æ„åˆ†æ
â””â”€â”€ requirements.txt              # ä¾èµ–
```

---

## ğŸ”§ æ ¸å¿ƒç»„ä»¶è¯´æ˜

### 1. Vision Encoder (`models/vision_encoder.py`)

**åŠŸèƒ½**: å°†å›¾åƒç¼–ç ä¸º token åºåˆ—

**æ”¯æŒçš„æ–¹æ¡ˆ**:
- **æ–¹æ¡ˆ A**: ç›´æ¥ patch embedding (æ— é¢„è®­ç»ƒ)
  ```python
  VisionConfig(
      use_vision_tower=False,
      img_size=224,
      patch_size=16
  )
  ```

- **æ–¹æ¡ˆ B**: é¢„è®­ç»ƒ vision tower
  ```python
  VisionConfig(
      use_vision_tower=True,
      vision_tower_name="facebook/dinov2-base",  # æˆ– CLIP, SigLIP
      freeze_vision_tower=True
  )
  ```

**è¾“å‡º**: `[B, N_patches, 768]` + spatial_info

### 2. Language Encoder (`models/language_encoder.py`)

**åŠŸèƒ½**: å°†æ–‡æœ¬æŒ‡ä»¤ç¼–ç ä¸º token åºåˆ—

**é…ç½®**:
```python
LanguageConfig(
    model_name="Qwen/Qwen3-0.6B-Base",  # è‡ªåŠ¨ fallback åˆ° Qwen2-0.5B
    max_length=77,
    freeze_encoder=True,
    output_dim=768
)
```

**è¾“å‡º**: `[B, L, 768]` + language_info

### 3. VGGT Backbone

**ä¸¤ç§å®ç°**:

#### A. VGGTAdapter (`models/vggt_adapter.py`)
- ä» HuggingFace åŠ è½½ `facebook/vggt`
- æ·»åŠ é€‚é…å±‚å¤„ç†è¾“å…¥è¾“å‡º
- æ”¯æŒå¤šæ¨¡æ€ token æ³¨å…¥

```python
VGGTConfig(
    use_pretrained_vggt=True,
    freeze_vggt=True,  # åªè®­ç»ƒé€‚é…å±‚
    embed_dim=768
)
```

#### B. SimpleVGGTBackbone (`models/vggt_backbone.py`)
- ç®€åŒ–å®ç°: Graph Conv + Self-Attention
- å¿«é€Ÿè®­ç»ƒå’Œå®éªŒ
- å®Œå…¨å¯è®­ç»ƒ

```python
VGGTConfig(
    use_pretrained_vggt=False,
    depth=6,
    num_heads=12,
    graph_type='grid'
)
```

**è¾“å‡º**: 
- `vision_features`: `[B, N_v, 768]`
- `language_features`: `[B, N_l, 768]`
- `global_features`: `[B, 16, 768]` (action queries)

### 4. Action Head (`models/action_head.py`)

**åŠŸèƒ½**: ä»å…¨å±€ç‰¹å¾é¢„æµ‹åŠ¨ä½œåºåˆ—

```python
ActionHeadConfig(
    input_dim=768,
    action_dim=7,  # (x, y, z, quat, gripper)
    action_horizon=10,  # é¢„æµ‹æœªæ¥ 10 æ­¥
    use_action_chunking=True
)
```

**è¾“å‡º**: `[B, T, action_dim]`

---

## ğŸš€ ä½¿ç”¨æŒ‡å—

### å¿«é€Ÿæµ‹è¯•æ¨¡å‹

```bash
cd vggt_vla

# æµ‹è¯•ç®€å•é…ç½®
python scripts/test_model.py --config simple

# æµ‹è¯• DINOv2 é…ç½®
python scripts/test_model.py --config dinov2
```

### è®­ç»ƒæ¨¡å‹

#### æ–¹å¼ 1: ä½¿ç”¨é¢„å®šä¹‰é…ç½®

```bash
# ç®€å•é…ç½® (æ¨èé¦–æ¬¡ä½¿ç”¨)
bash scripts/quick_start.sh configs/train_simple.yaml

# DINOv2 é…ç½®
bash scripts/quick_start.sh configs/train_with_dinov2.yaml

# å®Œæ•´é…ç½®
bash scripts/quick_start.sh configs/train_full.yaml
```

#### æ–¹å¼ 2: å‘½ä»¤è¡Œå‚æ•°

```bash
python scripts/train_vla.py \
  --dataset_repo lerobot/libero_spatial_image \
  --use_vision_tower \
  --vision_tower_name facebook/dinov2-base \
  --freeze_vision_tower \
  --language_model Qwen/Qwen3-0.6B-Base \
  --freeze_language \
  --use_pretrained_vggt \
  --freeze_vggt \
  --batch_size 16 \
  --num_epochs 100 \
  --lr 3e-5 \
  --log_dir ./logs \
  --exp_name my_experiment
```

### ç›‘æ§è®­ç»ƒ

```bash
# å¯åŠ¨ TensorBoard
tensorboard --logdir logs

# æµè§ˆå™¨è®¿é—®
# http://localhost:6006
```

---

## ğŸ“Š é…ç½®æ¨è

### åœºæ™¯ 1: å¿«é€Ÿå®éªŒå’Œè°ƒè¯•
```yaml
é…ç½®æ–‡ä»¶: configs/train_simple.yaml

ç‰¹ç‚¹:
- ç›´æ¥ patch embedding
- ç®€åŒ–ç‰ˆ VGGT
- Qwen2-0.5B
- å¿«é€Ÿè®­ç»ƒ (~50M å‚æ•°)

é€‚åˆ:
- éªŒè¯æ•°æ® pipeline
- å¿«é€Ÿè¿­ä»£
- èµ„æºæœ‰é™ç¯å¢ƒ
```

### åœºæ™¯ 2: å¹³è¡¡æ€§èƒ½å’Œé€Ÿåº¦
```yaml
é…ç½®æ–‡ä»¶: configs/train_with_dinov2.yaml

ç‰¹ç‚¹:
- DINOv2 vision tower
- ç®€åŒ–ç‰ˆ VGGT
- Qwen3-0.6B
- ä¸­ç­‰è®­ç»ƒæ—¶é—´ (~200M å‚æ•°)

é€‚åˆ:
- æ­£å¼å®éªŒ
- è®ºæ–‡åŸºçº¿
- å¤§å¤šæ•°åº”ç”¨
```

### åœºæ™¯ 3: æœ€ä½³æ€§èƒ½
```yaml
é…ç½®æ–‡ä»¶: configs/train_full.yaml

ç‰¹ç‚¹:
- DINOv2 vision tower
- facebook/vggt (é¢„è®­ç»ƒ)
- Qwen3-0.6B
- æ…¢é€Ÿè®­ç»ƒ (~500M å‚æ•°)

é€‚åˆ:
- æœ€ç»ˆæ¨¡å‹
- ç«èµ›æäº¤
- å……è¶³è®¡ç®—èµ„æº
```

---

## ğŸ” å¤šæ¨¡æ€å¤„ç†è¯¦è§£

### é—®é¢˜åˆ†æ

åŸå§‹ `vggt_vla` å®ç°çš„ä¸»è¦é—®é¢˜:

1. **VGGT ä¸æ˜¯ä» HuggingFace åŠ è½½**
   - ç°æœ‰: è‡ªå·±å®ç°çš„ç®€åŒ–ç‰ˆ
   - ç¼ºå¤±: facebook/vggt çš„é¢„è®­ç»ƒæƒé‡å’Œå®Œæ•´æ¶æ„

2. **è¾“å…¥è¾“å‡ºæ ¼å¼ä¸åŒ¹é…**
   - facebook/vggt æœŸæœ›: `[B, S, 3, H, W]` (è§†é¢‘)
   - VLA ä»»åŠ¡: `[B, 3, H, W]` (å•å¸§)
   - éœ€è¦é€‚é…å±‚

3. **Language æ³¨å…¥æ–¹å¼**
   - åŸå§‹ VGGT æ—  language è¾“å…¥
   - éœ€è¦è®¾è®¡å¤šæ¨¡æ€èåˆç­–ç•¥

### è§£å†³æ–¹æ¡ˆ

#### 1. VGGT Adapter
```python
# models/vggt_adapter.py

class VGGTAdapter:
    - åŠ è½½ facebook/vggt
    - é€‚é…å•å¸§è¾“å…¥
    - æ³¨å…¥ language tokens
    - æ·»åŠ  action queries
    - ç‰¹å¾æŠ•å½±å±‚
```

#### 2. Token Fusion
```python
Vision tokens: [B, 196, 768]
Language tokens: [B, 77, 768]
  â†“
Token Type Embeddings:
  - Vision: type=0
  - Language: type=1
  â†“
Concat: [B, 273, 768]
  â†“
VGGT Processing:
  - Graph Conv (intra-modal)
  - Self-Attention (cross-modal)
```

#### 3. Graph Structure
```python
Vision: Grid graph (2D spatial)
  - 14Ã—14 patches
  - 4-connectivity

Language: Chain graph (sequential)
  - 1D sequence
  - Bidirectional

Cross-modal: Through attention
  - No graph edges
  - Full attention matrix
```

---

## ğŸ“ˆ æ€§èƒ½ä¼˜åŒ–å»ºè®®

### 1. æ•°æ®åŠ è½½
```python
# å¢åŠ  workers
num_workers=8

# Pin memory
pin_memory=True

# Prefetch
persistent_workers=True
```

### 2. è®­ç»ƒä¼˜åŒ–
```python
# æ··åˆç²¾åº¦
from torch.cuda.amp import autocast, GradScaler

# æ¢¯åº¦ç´¯ç§¯
accumulation_steps=4

# æ¢¯åº¦æ£€æŸ¥ç‚¹ (èŠ‚çœå†…å­˜)
use_gradient_checkpointing=True
```

### 3. æ¨¡å‹ä¼˜åŒ–
```python
# ç¼–è¯‘æ¨¡å‹ (PyTorch 2.0+)
model = torch.compile(model)

# å†»ç»“é¢„è®­ç»ƒæ¨¡å‹
freeze_vision_tower=True
freeze_language=True
freeze_vggt=True
```

---

## ğŸ› å¸¸è§é—®é¢˜å’Œè§£å†³æ–¹æ¡ˆ

### Q1: CUDA Out of Memory

**è§£å†³æ–¹æ¡ˆ**:
```bash
# æ–¹æ³• 1: å‡å° batch size
--batch_size 8

# æ–¹æ³• 2: ä½¿ç”¨æ¢¯åº¦ç´¯ç§¯
--batch_size 8 --accumulation_steps 4  # ç­‰æ•ˆ batch_size=32

# æ–¹æ³• 3: ä½¿ç”¨æ›´å°çš„æ¨¡å‹
--use_vision_tower false

# æ–¹æ³• 4: å†»ç»“æ›´å¤šå‚æ•°
--freeze_vision_tower --freeze_language --freeze_vggt
```

### Q2: æ— æ³•åŠ è½½ Qwen3-0.6B

**è‡ªåŠ¨ Fallback**:
```python
# language_encoder.py ä¸­å·²å®ç°è‡ªåŠ¨ fallback
try:
    model = AutoModel.from_pretrained("Qwen/Qwen3-0.6B-Base")
except:
    model = AutoModel.from_pretrained("Qwen/Qwen2-0.5B")
```

**æ‰‹åŠ¨æŒ‡å®š**:
```bash
--language_model Qwen/Qwen2-0.5B
```

### Q3: è®­ç»ƒä¸æ”¶æ•›

**è¯Šæ–­æ­¥éª¤**:
```bash
# 1. æ£€æŸ¥æ•°æ®
python -c "from data.libero_hf_dataset import *; ..."

# 2. é™ä½å­¦ä¹ ç‡
--lr 1e-5

# 3. æ£€æŸ¥æ¢¯åº¦
# åœ¨ trainer.py ä¸­æ·»åŠ :
grad_norm = torch.nn.utils.clip_grad_norm_(model.parameters(), grad_clip)
print(f"Grad norm: {grad_norm}")

# 4. å¯è§†åŒ–
tensorboard --logdir logs
```

### Q4: facebook/vggt åŠ è½½å¤±è´¥

**Fallback åˆ°ç®€åŒ–ç‰ˆ**:
```python
# vggt_adapter.py ä¸­å·²å®ç°
try:
    vggt = AutoModel.from_pretrained("facebook/vggt")
except:
    from vggt.models.vggt import VGGT
    vggt = VGGT(...)  # ä½¿ç”¨æœ¬åœ°å®ç°
```

æˆ–ç›´æ¥ä½¿ç”¨ç®€åŒ–ç‰ˆ:
```bash
--use_pretrained_vggt false
```

---

## ğŸ“¦ ä¾èµ–å®‰è£…

### åŸºç¡€ä¾èµ–
```bash
pip install -r vggt_vla/requirements.txt
```

### å¯é€‰ä¾èµ–

#### 1. åŸå§‹ VGGT
```bash
cd vggt
pip install -e .
```

#### 2. Vision Towers
```bash
# DINOv2
pip install timm

# CLIP/SigLIP
# (å·²åŒ…å«åœ¨ transformers ä¸­)
```

#### 3. åˆ†å¸ƒå¼è®­ç»ƒ
```bash
pip install accelerate
```

---

## ğŸ¯ ä¸‹ä¸€æ­¥

### çŸ­æœŸç›®æ ‡
1. âœ… å®Œæˆæ¶æ„å®ç°
2. âœ… éªŒè¯æ¨¡å‹å¯ä»¥è¿è¡Œ
3. â³ åœ¨ LIBERO æ•°æ®é›†ä¸Šè®­ç»ƒ
4. â³ è¯„ä¼°æ€§èƒ½å’Œå¯¹æ¯” baseline

### ä¸­æœŸç›®æ ‡
1. æ¢ç´¢æ›´å¤šèåˆç­–ç•¥ (cross-attention, gating)
2. æ”¯æŒæ›´å¤š vision towers (SAM, EVA-CLIP)
3. å¤šä»»åŠ¡å­¦ä¹ 
4. æ¨¡å‹å‹ç¼©å’Œé‡åŒ–

### é•¿æœŸç›®æ ‡
1. çœŸå®æœºå™¨äººè¯„ä¼°
2. å¼€æºé¢„è®­ç»ƒæ¨¡å‹
3. è®ºæ–‡å‘è¡¨
4. ç¤¾åŒºè´¡çŒ®

---

## ğŸ“š å‚è€ƒèµ„æ–™

### è®ºæ–‡
- [VGGT: Visual Geometry Grounded Transformer](https://arxiv.org/abs/2403.08493)
- [LIBERO: Benchmarking Knowledge Transfer for Lifelong Robot Learning](https://arxiv.org/abs/2306.03310)

### ä»£ç 
- [facebook/vggt](https://huggingface.co/facebook/vggt)
- [LIBERO Dataset](https://huggingface.co/datasets/lerobot/libero_spatial_image)
- [Qwen3](https://huggingface.co/Qwen/Qwen3-0.6B-Base)

### æ–‡æ¡£
- [vggt_vla/README.md](./vggt_vla/README.md) - ç”¨æˆ·æ–‡æ¡£
- [vggt_vla/ARCHITECTURE_ANALYSIS.md](./vggt_vla/ARCHITECTURE_ANALYSIS.md) - æ¶æ„åˆ†æ

---

## ğŸ‘¥ è´¡çŒ®

æ¬¢è¿æäº¤ Issue å’Œ Pull Request!

---

## ğŸ“„ License

See LICENSE file.

---

**æœ€åæ›´æ–°**: 2024-02-11  
**ç‰ˆæœ¬**: 1.0  
**ä½œè€…**: VLA-VGGT Team
