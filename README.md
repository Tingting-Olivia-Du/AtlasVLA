# AtlasVLA

Vision-Language-Action (VLA) model for robot manipulation tasks, built on top of [VGGT](https://github.com/facebookresearch/vggt).

## Overview

AtlasVLA integrates 3D geometric understanding from VGGT with language instructions to predict robot actions. The model combines:

- **VGGT Backbone**: Extracts rich 3D geometric information (depth maps, point clouds, camera poses) from images
- **Language Encoder**: Processes natural language instructions using LLaMA 2 encoder
- **Multimodal Fusion**: Cross-attention mechanism to fuse language and 3D geometry
- **Action Prediction**: Outputs end-effector pose (6-DOF) and gripper actions

## Features

- ğŸ¯ **3D-Aware**: Leverages VGGT's powerful 3D scene understanding
- ğŸ—£ï¸ **Language-Conditioned**: Understands natural language task descriptions
- ğŸ¤– **Action Prediction**: Directly predicts robot actions for manipulation
- ğŸ“Š **LIBERO Support**: Ready-to-use training on LIBERO manipulation dataset
- ğŸ”§ **Flexible**: Supports freezing/unfreezing different components for efficient training

## Installation

### Prerequisites

- Python >= 3.8
- PyTorch >= 2.0.0
- CUDA-capable GPU (recommended)

### Install from Source

```bash
# Clone the repository
git clone https://github.com/yourusername/AtlasVLA.git
cd AtlasVLA

# Install in development mode
pip install -e .

# Or install with optional dependencies
pip install -e ".[wandb]"  # For wandb experiment tracking
pip install -e ".[dev]"    # For development tools
```

### Manual Installation

```bash
# Install dependencies
pip install -r requirements.txt

# Install VGGT as a package (optional, for better import handling)
pip install -e vggt/
```

## Quick Start

### Basic Usage

```python
import torch
from atlas.src.models import VGGTVLA

# Initialize model
model = VGGTVLA(
    lang_encoder_name="meta-llama/Llama-2-7b-hf",
    freeze_vggt=True,
    freeze_lang_encoder=False
)
model = model.to("cuda")

# Prepare inputs
images = torch.randn(1, 2, 3, 518, 518).to("cuda")  # [B, S, 3, H, W]
language_instruction = ["Pick up the red block"]

# Forward pass
outputs = model(images, language_instruction)
action = outputs["action"]  # [B, 7] - 6-DOF pose + gripper
pose = outputs["pose"]      # [B, 6]
gripper = outputs["gripper"]  # [B, 1]
```

### Training on LIBERO

1. **Prepare your data**: Download and organize LIBERO dataset (see [Training Guide](atlas/README_TRAINING.md))

2. **Configure training**: Edit `atlas/configs/train_config.yaml`

3. **Start training**:
```bash
python atlas/train.py --config atlas/configs/train_config.yaml
```

4. **Evaluate**:
```bash
python atlas/eval.py \
  --config atlas/configs/train_config.yaml \
  --checkpoint checkpoints/best_model.pt \
  --split val
```

## Project Structure

```
AtlasVLA/
â”œâ”€â”€ atlas/                    # Main Atlas VLA code
â”‚   â”œâ”€â”€ src/
â”‚   â”‚   â”œâ”€â”€ models/          # Model definitions
â”‚   â”‚   â”œâ”€â”€ data/            # Data loaders
â”‚   â”‚   â””â”€â”€ training/        # Training utilities
â”‚   â”œâ”€â”€ configs/             # Configuration files
â”‚   â”œâ”€â”€ train.py             # Training script
â”‚   â”œâ”€â”€ eval.py              # Evaluation script
â”‚   â””â”€â”€ README_TRAINING.md   # Detailed training guide
â”‚
â”œâ”€â”€ vggt/                    # VGGT dependency (submodule)
â”‚   â””â”€â”€ ...                  # VGGT original code
â”‚
â”œâ”€â”€ setup.py                 # Package installation
â”œâ”€â”€ pyproject.toml           # Modern Python project config
â”œâ”€â”€ requirements.txt         # Dependencies
â””â”€â”€ README.md                # This file
```

## Model Architecture

```
Input: RGB Images [B, S, 3, H, W] + Language Instructions
  â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  VGGT Backbone (frozen or trainable)   â”‚
â”‚  - Aggregator extracts visual features â”‚
â”‚  - Outputs: 3D geometry information     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
  â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  3D Geometry Feature Extractor          â”‚
â”‚  - Token features from VGGT            â”‚
â”‚  - Optional: Point cloud features      â”‚
â”‚  - Optional: Camera pose encoding      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
  â†“                    â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Language    â”‚  â”‚  3D Geometry â”‚
â”‚  Encoder     â”‚  â”‚  Features    â”‚
â”‚  (LLaMA 2)   â”‚  â”‚              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
  â†“                    â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Multimodal Fusion (Cross-Attention)    â”‚
â”‚  - Language queries attend to geometry  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
  â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Action Head                            â”‚
â”‚  - End-effector pose (6-DOF)            â”‚
â”‚  - Gripper action                       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## Training Strategy

### Phase 1: Freeze VGGT (Recommended Start)
- Freeze VGGT backbone
- Train only fusion and action head
- Fast training, low memory usage

### Phase 2: Unfreeze Language Encoder
- Fine-tune language encoder
- Better task-specific understanding

### Phase 3: End-to-End (Optional)
- Unfreeze VGGT
- Requires more GPU memory
- May improve performance

## Configuration

Key configuration options in `atlas/configs/train_config.yaml`:

- **Model**: VGGT checkpoint, language encoder, freeze options
- **Data**: Dataset path, batch size, image size
- **Training**: Learning rate, epochs, loss weights
- **Checkpointing**: Save directory, intervals

## Datasets

Currently supports:
- **LIBERO**: Manipulation benchmark with 130 tasks
  - RGB images (workspace + wrist cameras)
  - 7-DOF actions (6-DOF pose + gripper)
  - Language task descriptions

## Citation

If you use AtlasVLA in your research, please cite:

```bibtex
@misc{atlasvla2025,
  title={AtlasVLA: Vision-Language-Action Model with 3D Geometric Understanding},
  author={Your Name},
  year={2025},
  howpublished={\url{https://github.com/yourusername/AtlasVLA}}
}
```

And the original VGGT paper:

```bibtex
@inproceedings{wang2025vggt,
  title={VGGT: Visual Geometry Grounded Transformer},
  author={Wang, Jianyuan and Chen, Minghao and Karaev, Nikita and Vedaldi, Andrea and Rupprecht, Christian and Novotny, David},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  year={2025}
}
```

## License

This project follows the VGGT license. See [LICENSE](LICENSE) for details.

## Acknowledgments

- Built on [VGGT](https://github.com/facebookresearch/vggt) by Meta AI
- Uses [LLaMA 2](https://ai.meta.com/llama/) for language encoding
- Trained on [LIBERO](https://libero-project.github.io/) manipulation dataset

## Contributing

Contributions are welcome! Please feel free to submit a Pull Request.

## Contact

For questions or issues, please open an issue on GitHub.
