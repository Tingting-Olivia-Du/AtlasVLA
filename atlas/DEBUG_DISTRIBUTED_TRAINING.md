# åˆ†å¸ƒå¼è®­ç»ƒè°ƒè¯•æŒ‡å—

## ğŸ” æ®µé”™è¯¯ï¼ˆSegmentation Faultï¼‰å¸¸è§åŸå› 

### 1. NCCLåˆå§‹åŒ–é—®é¢˜

**ç—‡çŠ¶**: åœ¨ `dist.init_process_group()` æ—¶å´©æºƒ

**è§£å†³æ–¹æ¡ˆ**:
- âœ… ç¡®ä¿åœ¨åˆå§‹åŒ–å‰è®¾ç½®CUDAè®¾å¤‡: `torch.cuda.set_device(local_rank)`
- âœ… æ·»åŠ è¶…æ—¶è®¾ç½®é¿å…æ­»é”
- âœ… æ£€æŸ¥NCCLç¯å¢ƒå˜é‡

### 2. CUDAè®¾å¤‡é—®é¢˜

**æ£€æŸ¥**:
```bash
# æ£€æŸ¥CUDAæ˜¯å¦å¯ç”¨
python3 -c "import torch; print(torch.cuda.is_available()); print(torch.cuda.device_count())"

# æ£€æŸ¥GPUå¯è§æ€§
echo $CUDA_VISIBLE_DEVICES

# æ£€æŸ¥NCCL
python3 -c "import torch.distributed; print('NCCL available:', torch.distributed.is_nccl_available())"
```

### 3. å†…å­˜é—®é¢˜

**ç—‡çŠ¶**: OOMæˆ–æ®µé”™è¯¯

**è§£å†³æ–¹æ¡ˆ**:
- å‡å°‘batch_size
- å‡å°‘num_workers
- ä½¿ç”¨gradient checkpointing
- æ£€æŸ¥GPUå†…å­˜: `nvidia-smi`

### 4. ç¯å¢ƒå˜é‡é—®é¢˜

**æ£€æŸ¥å¿…è¦çš„ç¯å¢ƒå˜é‡**:
```bash
# torchrunä¼šè‡ªåŠ¨è®¾ç½®è¿™äº›ï¼Œä½†å¯ä»¥æ‰‹åŠ¨æ£€æŸ¥
echo $RANK
echo $WORLD_SIZE
echo $LOCAL_RANK
echo $MASTER_ADDR
echo $MASTER_PORT
```

## ğŸ› ï¸ è°ƒè¯•æ­¥éª¤

### æ­¥éª¤1: å•GPUæµ‹è¯•

é¦–å…ˆç¡®ä¿å•GPUè®­ç»ƒæ­£å¸¸ï¼š

```bash
CUDA_VISIBLE_DEVICES=0 python3 atlas/train.py --config atlas/configs/train_config.yaml
```

### æ­¥éª¤2: åŒGPUæµ‹è¯•

å¦‚æœå•GPUæ­£å¸¸ï¼Œæµ‹è¯•åŒGPUï¼š

```bash
CUDA_VISIBLE_DEVICES=0,1 torchrun --nproc_per_node=2 atlas/train.py --config atlas/configs/train_config.yaml
```

### æ­¥éª¤3: æ·»åŠ è°ƒè¯•è¾“å‡º

åœ¨ä»£ç ä¸­æ·»åŠ æ›´å¤šprintè¯­å¥æ¥å®šä½é—®é¢˜ï¼š

```python
print(f"[Rank {rank}] Before model init")
model = VGGTVLA(...)
print(f"[Rank {rank}] After model init")
```

### æ­¥éª¤4: æ£€æŸ¥æ—¥å¿—

æŸ¥çœ‹æ—¥å¿—æ–‡ä»¶ä¸­çš„é”™è¯¯ä¿¡æ¯ï¼š

```bash
tail -f logs/train_*.log
```

## ğŸ”§ ä¿®å¤åçš„æ”¹è¿›

### 1. æ”¹è¿›çš„åˆ†å¸ƒå¼åˆå§‹åŒ–

```python
# åœ¨init_process_groupä¹‹å‰è®¾ç½®è®¾å¤‡
torch.cuda.set_device(local_rank)

# æ·»åŠ è¶…æ—¶é¿å…æ­»é”
dist.init_process_group(
    backend='nccl',
    init_method='env://',
    timeout=timedelta(seconds=1800)  # 30åˆ†é’Ÿ
)
```

### 2. é”™è¯¯å¤„ç†

æ·»åŠ äº†try-catchå—æ¥æ•è·å’Œè®°å½•é”™è¯¯ï¼š

```python
try:
    dist.init_process_group(...)
except Exception as e:
    logging.error(f"Error: {e}")
    raise
```

### 3. DDPä¼˜åŒ–

```python
model = DDP(
    model,
    device_ids=[local_rank],
    output_device=local_rank,
    find_unused_parameters=True,
    broadcast_buffers=True,
    gradient_as_bucket_view=True  # æ›´èŠ‚çœå†…å­˜
)
```

## ğŸ“‹ å¸¸è§é”™è¯¯å’Œè§£å†³æ–¹æ¡ˆ

### é”™è¯¯1: "NCCL error: unhandled system error"

**åŸå› **: NCCLé€šä¿¡é—®é¢˜

**è§£å†³**:
```bash
# è®¾ç½®NCCLè°ƒè¯•
export NCCL_DEBUG=INFO
export NCCL_DEBUG_SUBSYS=ALL

# æˆ–ä½¿ç”¨TCPåç«¯ï¼ˆå¦‚æœNCCLæœ‰é—®é¢˜ï¼‰
# ä¿®æ”¹ä»£ç : backend='gloo'  # ä½†glooä¸æ”¯æŒCUDAï¼Œåªç”¨äºè°ƒè¯•
```

### é”™è¯¯2: "CUDA out of memory"

**è§£å†³**:
- å‡å°‘batch_size
- å‡å°‘num_workers
- ä½¿ç”¨gradient accumulation
- æ£€æŸ¥æ˜¯å¦æœ‰å…¶ä»–è¿›ç¨‹å ç”¨GPU

### é”™è¯¯3: "Address already in use"

**åŸå› **: MASTER_PORTè¢«å ç”¨

**è§£å†³**:
```bash
# ä½¿ç”¨ä¸åŒçš„ç«¯å£
export MASTER_PORT=29501

# æˆ–è®©torchrunè‡ªåŠ¨é€‰æ‹©
# torchrunä¼šè‡ªåŠ¨å¤„ç†ç«¯å£å†²çª
```

## ğŸš€ æ¨èçš„è®­ç»ƒå‘½ä»¤

### å•GPUï¼ˆè°ƒè¯•ç”¨ï¼‰

```bash
CUDA_VISIBLE_DEVICES=0 python3 atlas/train.py --config atlas/configs/train_config.yaml
```

### å¤šGPUï¼ˆç”Ÿäº§ç”¨ï¼‰

```bash
# ä½¿ç”¨train.shè„šæœ¬ï¼ˆæ¨èï¼‰
CUDA_VISIBLE_DEVICES=0,1,2,3 ./atlas/scripts/train.sh

# æˆ–ç›´æ¥ä½¿ç”¨torchrun
torchrun --nproc_per_node=4 atlas/train.py --config atlas/configs/train_config.yaml
```

### å¸¦è°ƒè¯•ä¿¡æ¯

```bash
# å¯ç”¨NCCLè°ƒè¯•
export NCCL_DEBUG=INFO
export NCCL_DEBUG_SUBSYS=ALL

# è¿è¡Œè®­ç»ƒ
torchrun --nproc_per_node=4 atlas/train.py --config atlas/configs/train_config.yaml
```

## ğŸ“Š ç›‘æ§è®­ç»ƒ

### å®æ—¶ç›‘æ§GPU

```bash
# å¦ä¸€ä¸ªç»ˆç«¯
watch -n 1 nvidia-smi
```

### æŸ¥çœ‹è¿›ç¨‹

```bash
# æŸ¥çœ‹è®­ç»ƒè¿›ç¨‹
ps aux | grep train.py

# æŸ¥çœ‹GPUè¿›ç¨‹
nvidia-smi
```

## ğŸ’¡ æœ€ä½³å®è·µ

1. **å…ˆå•GPUæµ‹è¯•**: ç¡®ä¿ä»£ç é€»è¾‘æ­£ç¡®
2. **é€æ­¥å¢åŠ GPU**: ä»2ä¸ªGPUå¼€å§‹ï¼Œé€æ­¥å¢åŠ åˆ°8ä¸ª
3. **ç›‘æ§èµ„æº**: ä½¿ç”¨nvidia-smiç›‘æ§GPUä½¿ç”¨
4. **ä¿å­˜æ—¥å¿—**: å¯ç”¨æ—¥å¿—ä¿å­˜ä»¥ä¾¿è°ƒè¯•
5. **ä½¿ç”¨wandb**: å¯è§†åŒ–è®­ç»ƒè¿‡ç¨‹

## ğŸ”— ç›¸å…³æ–‡æ¡£

- PyTorchåˆ†å¸ƒå¼è®­ç»ƒ: https://pytorch.org/tutorials/intermediate/ddp_tutorial.html
- NCCLæ–‡æ¡£: https://docs.nvidia.com/deeplearning/nccl/
- è®­ç»ƒè„šæœ¬ä½¿ç”¨: `atlas/scripts/train.sh`
