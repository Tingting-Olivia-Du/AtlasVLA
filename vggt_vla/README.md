# VLA-VGGT: Vision-Language-Action Model with VGGT Backbone

Vision-Language-Action model for robotic manipulation using VGGT (Vision GNN Transformer) as the backbone.

> ğŸ¯ **å¿«é€Ÿå¼€å§‹**: ä½¿ç”¨ facebook/vggt + Qwen3-0.6B-Baseï¼ŸæŸ¥çœ‹ [VGGT_QWEN3_GUIDE.md](./VGGT_QWEN3_GUIDE.md)

> ğŸ“– **å®Œæ•´æ–‡æ¡£**: æŸ¥çœ‹ [ARCHITECTURE_ANALYSIS.md](./ARCHITECTURE_ANALYSIS.md) äº†è§£è¯¦ç»†çš„æ¶æ„åˆ†æå’Œå¤šæ¨¡æ€å¤„ç†è¯´æ˜

## âœ¨ æ ¸å¿ƒç‰¹æ€§

- **çµæ´»çš„ Vision Encoder**: 
  - ç›´æ¥ patch embedding (å¿«é€Ÿå®éªŒ)
  - é¢„è®­ç»ƒ vision tower (DINOv2/CLIP/SigLIP)
- **Language Encoder**: Qwen3-0.6B-Base (with fallback to Qwen2-0.5B)
- **VGGT Backbone**: 
  - facebook/vggt from HuggingFace (é¢„è®­ç»ƒæƒé‡)
  - ç®€åŒ–ç‰ˆ VGGT (å¿«é€Ÿè®­ç»ƒ)
- **Action Head**: MLP with action chunking
- **æ•°æ®é›†**: HuggingFace LIBERO datasets (lerobot/libero_spatial_image)

## å¤šæ¨¡æ€ Token å¤„ç†

### Vision Tokens
- Image [B,3,224,224] â†’ Patch Embedding â†’ [B,196,768]
- 196 = 14Ã—14 patches (æ¯ä¸ª patch æ˜¯ 16Ã—16 pixels)
- è™½ç„¶å˜æˆ 1D sequenceï¼Œä½†é€šè¿‡ 2D positional encoding å’Œ grid graph ä¿ç•™ç©ºé—´ä¿¡æ¯

### Language Tokens
- Text â†’ Qwen2 â†’ [B,L,1024] â†’ Projector â†’ [B,L,768]
- 1D sequence structure
- Chain graph è¿æ¥

### Token Fusion
- æ‹¼æ¥: [Language Tokens | Vision Tokens]
- Token Type Embeddings åŒºåˆ†æ¨¡æ€
- Graph Structure:
  - Language: Chain graph (sequential)
  - Vision: Grid graph (spatial 2D)
  - Cross-modal: Through attention, not graph edges

## ğŸ“¦ å®‰è£…

```bash
# 1. è¿›å…¥ç›®å½•
cd vggt_vla

# 2. å®‰è£…ä¾èµ–
pip install -r requirements.txt

# 3. æ›´æ–° transformers (Qwen3 éœ€è¦)
pip install -U "transformers>=4.51.0"

# 4. (å¯é€‰) å®‰è£…åŸå§‹ VGGT (ç”¨äºæœ¬åœ° fallback)
cd ../vggt
pip install -e .
cd ../vggt_vla
```

## ğŸš€ å¿«é€Ÿå¼€å§‹

### æ¨èé…ç½®: facebook/vggt + Qwen3-0.6B-Base (å•å¸§è¾“å…¥)

```bash
# 1. æµ‹è¯•æ¨¡å‹
python scripts/test_vggt_qwen3.py

# 2. å¼€å§‹è®­ç»ƒ - åŸºç¡€é…ç½®
bash scripts/quick_start.sh configs/train_vggt_qwen3.yaml

# 3. æˆ–ä½¿ç”¨å®Œæ•´é…ç½® (+ DINOv2)
bash scripts/quick_start.sh configs/train_vggt_qwen3_dinov2.yaml
```

> ğŸ“– è¯¦ç»†è¯´æ˜: [VGGT_QWEN3_GUIDE.md](./VGGT_QWEN3_GUIDE.md)

### å…¶ä»–é…ç½®

```bash
# ç®€å•é…ç½® - å¿«é€Ÿå®éªŒ
bash scripts/quick_start.sh configs/train_simple.yaml

# ä½¿ç”¨ DINOv2 vision tower
bash scripts/quick_start.sh configs/train_with_dinov2.yaml

# å®Œæ•´é…ç½® - æœ€ä½³æ€§èƒ½
bash scripts/quick_start.sh configs/train_full.yaml
```

### æ–¹å¼ 2: å‘½ä»¤è¡Œå‚æ•°

```bash
python scripts/train_vla.py \
    --dataset_repo lerobot/libero_spatial_image \
    --use_vision_tower \
    --vision_tower_name facebook/dinov2-base \
    --language_model Qwen/Qwen3-0.6B-Base \
    --freeze_language \
    --batch_size 24 \
    --num_epochs 100 \
    --lr 5e-5 \
    --log_dir ./logs \
    --exp_name my_experiment
```

### ç›‘æ§è®­ç»ƒ

åœ¨é…ç½®ä¸­è®¾ç½® `use_wandb: true`ï¼Œè®­ç»ƒæ—¶è‡ªåŠ¨ä¸ŠæŠ¥åˆ° [Weights & Biases](https://wandb.ai)ã€‚ä¹Ÿå¯æŸ¥çœ‹ `log_dir` ä¸‹çš„ `train_*.log` æ–‡æœ¬æ—¥å¿—ã€‚

## é¡¹ç›®ç»“æ„
```
vla_vggt_project/
â”œâ”€â”€ configs/          # é…ç½®æ–‡ä»¶
â”œâ”€â”€ models/           # æ¨¡å‹å®ç°
â”‚   â”œâ”€â”€ components/   # VGGT æ ¸å¿ƒç»„ä»¶
â”‚   â”œâ”€â”€ vision_encoder.py
â”‚   â”œâ”€â”€ language_encoder.py
â”‚   â”œâ”€â”€ vggt_backbone.py
â”‚   â”œâ”€â”€ action_head.py
â”‚   â””â”€â”€ vla_model.py
â”œâ”€â”€ data/             # æ•°æ®åŠ è½½
â”œâ”€â”€ training/         # è®­ç»ƒå·¥å…·
â””â”€â”€ scripts/          # è®­ç»ƒ/è¯„ä¼°è„šæœ¬
```

## å…³é”®å®ç°ç»†èŠ‚

### 2D â†’ 1D ä½†ä¿ç•™ç©ºé—´ä¿¡æ¯

è™½ç„¶ vision tokens ä» 2D grid å˜æˆäº† 1D sequenceï¼Œä½†ç©ºé—´ä¿¡æ¯é€šè¿‡ä»¥ä¸‹æ–¹å¼ä¿ç•™:

1. **2D Positional Encoding**: ä¸ºæ¯ä¸ª patch ç¼–ç å…¶ (row, col) ä½ç½®
2. **Grid Graph**: æ˜¾å¼è¿æ¥ç©ºé—´é‚»å±… (4-connectivity æˆ– 8-connectivity)
3. **Spatial Info Dict**: è®°å½• patch_positions [196, 2] ç”¨äº graph æ„å»º

### VGGT Layer å¤„ç†

æ¯ä¸ª VGGT layer åŒ…å«:
1. **Graph Convolution**: åŸºäº graph edges çš„å±€éƒ¨ä¿¡æ¯èšåˆ
2. **Self-Attention**: å…¨å±€çš„ token-to-token interaction
3. **FFN**: ç‰¹å¾å˜æ¢

è¿™æ ·è®¾è®¡ä½¿å¾—:
- Graph Conv å¤„ç† intra-modal ç»“æ„ (language chain, vision grid)
- Attention å¤„ç† cross-modal äº¤äº’

## é…ç½®è¯´æ˜

ç¼–è¾‘ `configs/model_config.py` æ¥è‡ªå®šä¹‰:
- æ¨¡å‹ç»´åº¦
- VGGT å±‚æ•°
- Graph ç»“æ„ç±»å‹
- Action head å‚æ•°

## ğŸ“Š é…ç½®é€‰é¡¹

### é¢„å®šä¹‰é…ç½®

| é…ç½® | Vision | VGGT | å‚æ•°é‡ | é€Ÿåº¦ | é€‚ç”¨åœºæ™¯ |
|------|--------|------|--------|------|----------|
| `train_simple.yaml` | Patch Embed | Simple | ~50M | å¿« | å¿«é€Ÿå®éªŒ |
| `train_with_dinov2.yaml` | DINOv2 | Simple | ~200M | ä¸­ | å¹³è¡¡æ€§èƒ½ |
| `train_full.yaml` | DINOv2 | facebook/vggt | ~500M | æ…¢ | æœ€ä½³æ€§èƒ½ |

### è‡ªå®šä¹‰é…ç½®

å¤åˆ¶å¹¶ä¿®æ”¹é…ç½®æ–‡ä»¶:
```bash
cp configs/train_simple.yaml configs/my_config.yaml
# ç¼–è¾‘ my_config.yaml
bash scripts/quick_start.sh configs/my_config.yaml
```

## ğŸ”§ æ¶æ„è¯¦è§£

è¯¦ç»†çš„æ¶æ„åˆ†æå’Œå¤šæ¨¡æ€å¤„ç†è¯´æ˜ï¼Œè¯·æŸ¥çœ‹ [ARCHITECTURE_ANALYSIS.md](./ARCHITECTURE_ANALYSIS.md)

å…³é”®æ”¹è¿›:
- âœ… æ”¯æŒ HuggingFace çš„ facebook/vggt
- âœ… é€‚é… Qwen3-0.6B-Base è¯­è¨€æ¨¡å‹
- âœ… çµæ´»çš„ vision tower é€‰é¡¹
- âœ… æ”¹è¿›çš„å¤šæ¨¡æ€èåˆç­–ç•¥
- âœ… HuggingFace datasets é›†æˆ

## ğŸ“ å¸¸è§é—®é¢˜

**Q: å†…å­˜ä¸è¶³?**
```bash
# å‡å° batch size æˆ–ä½¿ç”¨ç®€åŒ–é…ç½®
python scripts/train_vla.py --config configs/train_simple.yaml --batch_size 16
```

**Q: æ— æ³•åŠ è½½ Qwen3-0.6B?**
```bash
# è‡ªåŠ¨ fallback åˆ° Qwen2-0.5B
# æˆ–æ‰‹åŠ¨æŒ‡å®š: --language_model Qwen/Qwen2-0.5B
```

**Q: å¦‚ä½•ä½¿ç”¨æœ¬åœ°æ•°æ®é›†?**
```python
# ä¿®æ”¹ data/libero_dataset.py ä½¿ç”¨æœ¬åœ° HDF5
from data.libero_dataset import get_libero_dataloaders
train_loader, val_loader = get_libero_dataloaders(
    data_path="/path/to/local/data.hdf5",
    ...
)
```

æ›´å¤šé—®é¢˜? æŸ¥çœ‹ [ARCHITECTURE_ANALYSIS.md](./ARCHITECTURE_ANALYSIS.md#å¸¸è§é—®é¢˜)

## ğŸ“š ç›¸å…³èµ„æº

- [VGGT Paper](https://arxiv.org/abs/2403.08493)
- [LIBERO Dataset](https://huggingface.co/datasets/lerobot/libero_spatial_image)
- [Qwen3 Model](https://huggingface.co/Qwen/Qwen3-0.6B-Base)
- [DINOv2](https://huggingface.co/facebook/dinov2-base)

## ğŸ“„ Citation

```bibtex
@article{vla_vggt,
  title={Vision-Language-Action Model with VGGT Backbone},
  author={Your Name},
  year={2024}
}

@article{vggt,
  title={VGGT: Visual Geometry Grounded Transformer},
  author={Meta AI},
  year={2024}
}
```
