# ä½¿ç”¨ facebook/vggt + Qwen3-0.6B-Base çš„æŒ‡å—

## ğŸ“‹ é…ç½®è¯´æ˜

æ ¹æ®ä½ çš„è¦æ±‚ï¼Œæˆ‘å·²ç»å®Œæˆäº†ä»¥ä¸‹é…ç½®ï¼š

### âœ… 1. ä½¿ç”¨åŸå§‹çš„ facebook/vggt

**å®ç°ä½ç½®**: `models/vggt_adapter.py`

- âœ… ä» HuggingFace åŠ è½½ `facebook/vggt`
- âœ… å¦‚æœæ— æ³•è®¿é—® HuggingFaceï¼Œè‡ªåŠ¨ fallback åˆ°æœ¬åœ° vggt å®ç°
- âœ… é€‚é…å±‚å¤„ç†è¾“å…¥è¾“å‡ºæ ¼å¼

**å…³é”®ä»£ç **:
```python
self.vggt = AutoModel.from_pretrained(
    "facebook/vggt",
    trust_remote_code=True
)
```

**é…ç½®**:
```yaml
use_pretrained_vggt: true  # ä½¿ç”¨ HuggingFace çš„ facebook/vggt
freeze_vggt: true          # å†»ç»“VGGTï¼Œåªè®­ç»ƒé€‚é…å±‚
```

### âœ… 2. å•å¸§è¾“å…¥ç»™ VGGT

**é—®é¢˜**: facebook/vggt åŸæœ¬è®¾è®¡ç”¨äºè§†é¢‘åºåˆ— `[B, S, 3, H, W]`ï¼Œå…¶ä¸­ S æ˜¯åºåˆ—é•¿åº¦

**è§£å†³æ–¹æ¡ˆ**: 

æˆ‘ä»¬çš„ VGGTAdapter å·²ç»å®ç°äº†å•å¸§å¤„ç†ï¼š

1. **è¾“å…¥æ ¼å¼**: æ¯æ¬¡ forward åªæ¥æ”¶ä¸€å¸§ `[B, 3, 224, 224]`
2. **å†…éƒ¨å¤„ç†**: Vision encoder å°†å›¾åƒè½¬æ¢ä¸º tokens `[B, 196, 768]`
3. **VGGT é€‚é…**: ä½¿ç”¨ VGGT çš„ aggregator blocks å¤„ç† tokens
4. **è¾“å‡º**: æå–é€‚åˆåŠ¨ä½œé¢„æµ‹çš„ç‰¹å¾

**å…³é”®å®ç°** (`models/vggt_adapter.py`):
```python
def forward(
    self,
    vision_tokens: torch.Tensor,      # [B, N_v, D] - å•å¸§çš„tokens
    language_tokens: torch.Tensor,    # [B, N_l, D]
    ...
):
    # å•å¸§å¤„ç†æµç¨‹
    # 1. é€‚é…ç»´åº¦
    vision_adapted = self.vision_adapter(vision_tokens)
    
    # 2. ä½¿ç”¨VGGTçš„transformer blocks
    aggregator = self.vggt.aggregator
    for i in range(num_layers):
        x = aggregator.frame_blocks[i](x, pos=None)
        x = aggregator.global_blocks[i](x, pos=None)
    
    # 3. æå–action features
    ...
```

**æ ‡è®°**: `output_info['single_frame_input'] = True`

### âœ… 3. ä½¿ç”¨ Qwen3-0.6B-Base ä½œä¸ºè¯­è¨€ encoder

**æ¨¡å‹**: [Qwen/Qwen3-0.6B-Base](https://huggingface.co/Qwen/Qwen3-0.6B-Base)

**ç‰¹ç‚¹**:
- 0.6B å‚æ•°
- 32K ä¸Šä¸‹æ–‡é•¿åº¦
- æ”¯æŒ 119 ç§è¯­è¨€
- Apache 2.0 è®¸å¯è¯

**å®ç°ä½ç½®**: `models/language_encoder.py`

**é…ç½®**:
```python
LanguageConfig(
    model_name="Qwen/Qwen3-0.6B-Base",  # âœ… ä½¿ç”¨ Qwen3
    freeze_encoder=True,                 # å†»ç»“encoder
    output_dim=768,                      # æŠ•å½±åˆ°768ç»´
    max_length=77                        # æœ€å¤§åºåˆ—é•¿åº¦
)
```

**è‡ªåŠ¨ Fallback**:
```python
try:
    model = AutoModel.from_pretrained("Qwen/Qwen3-0.6B-Base")
except:
    model = AutoModel.from_pretrained("Qwen/Qwen2-0.5B")  # Fallback
```

---

## ğŸš€ å¿«é€Ÿå¼€å§‹

### 1. æµ‹è¯•æ¨¡å‹

```bash
cd /workspace/tingting/AtlasVLA/vggt_vla

# æµ‹è¯• facebook/vggt + Qwen3-0.6B-Base é…ç½®
python scripts/test_vggt_qwen3.py
```

è¿™ä¼šéªŒè¯ï¼š
- âœ“ facebook/vggt åŠ è½½æˆåŠŸ
- âœ“ Qwen3-0.6B-Base é›†æˆæ­£ç¡®
- âœ“ å•å¸§è¾“å…¥å¤„ç†å·¥ä½œ
- âœ“ Action prediction æ­£å¸¸

### 2. å¼€å§‹è®­ç»ƒ

#### æ–¹æ¡ˆ A: åŸºç¡€é…ç½® (æ¨èé¦–æ¬¡ä½¿ç”¨)

```bash
# ä½¿ç”¨é…ç½®æ–‡ä»¶
bash scripts/quick_start.sh configs/train_vggt_qwen3.yaml
```

**ç‰¹ç‚¹**:
- facebook/vggt (å†»ç»“)
- Qwen3-0.6B-Base (å†»ç»“)
- ç›´æ¥ patch embedding (æ—  vision tower)
- å•å¸§è¾“å…¥
- åªè®­ç»ƒé€‚é…å±‚ (~50M å‚æ•°)
- Batch size: 16

#### æ–¹æ¡ˆ B: å®Œæ•´é…ç½® (æœ€ä½³æ€§èƒ½)

```bash
# ä½¿ç”¨ DINOv2 + facebook/vggt + Qwen3
bash scripts/quick_start.sh configs/train_vggt_qwen3_dinov2.yaml
```

**ç‰¹ç‚¹**:
- DINOv2 vision tower (å†»ç»“)
- facebook/vggt (å†»ç»“)
- Qwen3-0.6B-Base (å†»ç»“)
- å•å¸§è¾“å…¥
- åªè®­ç»ƒé€‚é…å±‚ (~80M å‚æ•°)
- Batch size: 12

### 3. ç›‘æ§è®­ç»ƒ

é…ç½®ä¸­è®¾ç½® `use_wandb: true`ï¼Œåœ¨ [wandb.ai](https://wandb.ai) æŸ¥çœ‹æ›²çº¿ï¼›æˆ–æŸ¥çœ‹ `log_dir` ä¸‹çš„ `train_*.log` æ–‡æœ¬æ—¥å¿—ã€‚

---

## ğŸ“Š é…ç½®å¯¹æ¯”

| é…ç½® | Vision | VGGT | Language | å‚æ•°é‡ | è®­ç»ƒé€Ÿåº¦ | é€‚ç”¨åœºæ™¯ |
|------|--------|------|----------|--------|----------|----------|
| train_vggt_qwen3.yaml | Patch Embed | facebook/vggt | Qwen3-0.6B | ~50M | å¿« | å¿«é€Ÿå®éªŒ |
| train_vggt_qwen3_dinov2.yaml | DINOv2 | facebook/vggt | Qwen3-0.6B | ~80M | ä¸­ | æœ€ä½³æ€§èƒ½ |

---

## ğŸ”§ æ¶æ„ç»†èŠ‚

### æ•´ä½“æµç¨‹

```
å•å¸§å›¾åƒ [B, 3, 224, 224]
    â†“
Vision Encoder (Patch Embed æˆ– DINOv2)
    â†“
Vision Tokens [B, 196, 768]
    â†“
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚ Vision Adapter  â”‚
                    â”‚ 768 â†’ 1024      â”‚
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â†“
                    [B, 196, 1024]
                            â”‚
                            â”‚
è¯­è¨€æŒ‡ä»¤ "pick up the red block"    â”‚
    â†“                              â”‚
Qwen3-0.6B-Base                    â”‚
    â†“                              â”‚
Language Tokens [B, 77, 1024]      â”‚
    â†“                              â”‚
Language Adapter                   â”‚
    â†“                              â”‚
[B, 77, 1024] â”€â”€â”€â”€â”€â”€Cross-Attentionâ”€â”˜
    â†“
[B, 77+196, 1024]
    â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ facebook/vggt                   â”‚
â”‚ - Alternating Attention         â”‚
â”‚ - Frame blocks + Global blocks  â”‚
â”‚ - Spatial reasoning             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â†“
[B, 273, 1024]
    â†“
Feature Projector (1024*2 â†’ 768)
    â†“
[B, 273, 768]
    â†“
    â”œâ”€ Vision Features [B, 196, 768]
    â””â”€ Language Features [B, 77, 768]
    â†“
Action Queries [B, 16, 768]
    â†“
Action Head
    â†“
Actions [B, 10, 7]
```

### å•å¸§å¤„ç†å…³é”®ç‚¹

1. **è¾“å…¥é€‚é…**:
   - å•å¸§å›¾åƒ â†’ Vision encoder â†’ Tokens
   - ä¸éœ€è¦æ„é€ è§†é¢‘åºåˆ—

2. **VGGT å¤„ç†**:
   - ä½¿ç”¨ VGGT çš„ aggregator blocks
   - Frame attention: å¤„ç†å½“å‰å¸§çš„tokens
   - Global attention: è·¨æ¨¡æ€äº¤äº’

3. **ç‰¹å¾æå–**:
   - Vision å’Œ language tokens èåˆ
   - Action queries èšåˆä¿¡æ¯
   - æŠ•å½±åˆ°åŠ¨ä½œç©ºé—´

---

## ğŸ¯ è®­ç»ƒå»ºè®®

### é¦–æ¬¡è®­ç»ƒ

1. **ä½¿ç”¨åŸºç¡€é…ç½®**: `train_vggt_qwen3.yaml`
2. **å°batch size**: ä» 8-16 å¼€å§‹
3. **çŸ­æœŸè®­ç»ƒ**: å…ˆè®­ç»ƒ 10-20 epochs éªŒè¯
4. **æ£€æŸ¥æŒ‡æ ‡**: TensorBoard ç›‘æ§ loss æ›²çº¿

### æ­£å¼è®­ç»ƒ

1. **ä½¿ç”¨å®Œæ•´é…ç½®**: `train_vggt_qwen3_dinov2.yaml`
2. **è°ƒæ•´å­¦ä¹ ç‡**: æ ¹æ®éªŒè¯é›†è¡¨ç°è°ƒæ•´
3. **å®šæœŸä¿å­˜**: æ¯ 5 epochs ä¿å­˜ checkpoint
4. **æ—©åœ**: å¦‚æœéªŒè¯ loss ä¸ä¸‹é™ï¼ŒåŠæ—¶åœæ­¢

### è°ƒè¯•æŠ€å·§

1. **æ£€æŸ¥ VGGT åŠ è½½**:
   ```bash
   python -c "from transformers import AutoModel; \
              model = AutoModel.from_pretrained('facebook/vggt', trust_remote_code=True); \
              print('âœ“ VGGT loaded')"
   ```

2. **æ£€æŸ¥ Qwen3 åŠ è½½**:
   ```bash
   python -c "from transformers import AutoModel; \
              model = AutoModel.from_pretrained('Qwen/Qwen3-0.6B-Base'); \
              print('âœ“ Qwen3 loaded')"
   ```

3. **æµ‹è¯•å•å¸§è¾“å…¥**:
   ```bash
   python scripts/test_vggt_qwen3.py
   ```

---

## ğŸ› å¸¸è§é—®é¢˜

### Q1: æ— æ³•åŠ è½½ facebook/vggt

**é—®é¢˜**: `Cannot load facebook/vggt from HuggingFace`

**è§£å†³**:
1. æ£€æŸ¥ç½‘ç»œè¿æ¥
2. ä½¿ç”¨æœ¬åœ° vggt:
   ```bash
   cd /workspace/tingting/AtlasVLA/vggt
   pip install -e .
   ```
3. è‡ªåŠ¨ fallback ä¼šä½¿ç”¨æœ¬åœ°å®ç°

### Q2: Qwen3-0.6B-Base åŠ è½½å¤±è´¥

**é—®é¢˜**: `KeyError: 'qwen3'`

**è§£å†³**:
1. æ›´æ–° transformers:
   ```bash
   pip install -U transformers>=4.51.0
   ```
2. æˆ–ä½¿ç”¨ fallback (Qwen2-0.5B)

### Q3: å†…å­˜ä¸è¶³

**é—®é¢˜**: `CUDA out of memory`

**è§£å†³**:
1. å‡å° batch size:
   ```bash
   --batch_size 8  # æˆ–æ›´å°
   ```
2. ä½¿ç”¨æ¢¯åº¦ç´¯ç§¯:
   ```python
   accumulation_steps = 4
   ```
3. ç¡®ä¿æ‰€æœ‰é¢„è®­ç»ƒæ¨¡å‹éƒ½å†»ç»“:
   ```yaml
   freeze_vggt: true
   freeze_language: true
   freeze_vision_tower: true  # å¦‚æœä½¿ç”¨
   ```

### Q4: è®­ç»ƒä¸æ”¶æ•›

**è¯Šæ–­**:
1. æ£€æŸ¥æ•°æ®åŠ è½½æ˜¯å¦æ­£å¸¸
2. é™ä½å­¦ä¹ ç‡: `--lr 1e-5`
3. æ£€æŸ¥æ¢¯åº¦: æ·»åŠ  `grad_norm` æ—¥å¿—
4. æŸ¥çœ‹ TensorBoard æ›²çº¿

---

## ğŸ“ˆ é¢„æœŸæ€§èƒ½

### è®­ç»ƒæ—¶é—´ (å• V100 GPU)

| é…ç½® | Epoch æ—¶é—´ | 100 Epochs | å¤‡æ³¨ |
|------|-----------|-----------|------|
| train_vggt_qwen3.yaml | ~15 min | ~25 å°æ—¶ | 1000 episodes |
| train_vggt_qwen3_dinov2.yaml | ~25 min | ~42 å°æ—¶ | 1000 episodes |

### å†…å­˜å ç”¨

| é…ç½® | Batch=16 | Batch=8 | Batch=4 |
|------|----------|---------|---------|
| train_vggt_qwen3.yaml | ~20 GB | ~12 GB | ~8 GB |
| train_vggt_qwen3_dinov2.yaml | ~28 GB | ~16 GB | ~10 GB |

---

## ğŸ“š ç›¸å…³èµ„æº

### æ¨¡å‹

- [facebook/vggt](https://huggingface.co/facebook/vggt) - Visual Geometry Grounded Transformer
- [Qwen3-0.6B-Base](https://huggingface.co/Qwen/Qwen3-0.6B-Base) - è¯­è¨€æ¨¡å‹
- [DINOv2-base](https://huggingface.co/facebook/dinov2-base) - Vision tower (å¯é€‰)

### æ•°æ®é›†

- [lerobot/libero_spatial_image](https://huggingface.co/datasets/lerobot/libero_spatial_image) - LIBERO æ•°æ®é›†

### è®ºæ–‡

- [VGGT Paper](https://arxiv.org/abs/2403.08493)
- [Qwen3 Technical Report](https://arxiv.org/abs/2505.09388)
- [LIBERO Paper](https://arxiv.org/abs/2306.03310)

---

## âœ… éªŒè¯æ¸…å•

åœ¨å¼€å§‹è®­ç»ƒå‰ï¼Œç¡®è®¤ï¼š

- [ ] å®‰è£…äº†ä¾èµ–: `pip install -r requirements.txt`
- [ ] æ›´æ–°äº† transformers: `pip install -U transformers>=4.51.0`
- [ ] æµ‹è¯•é€šè¿‡: `python scripts/test_vggt_qwen3.py`
- [ ] CUDA å¯ç”¨: `python -c "import torch; print(torch.cuda.is_available())"`
- [ ] æœ‰è¶³å¤Ÿç£ç›˜ç©ºé—´ (è‡³å°‘ 20GB for HuggingFace cache)
- [ ] é€‰æ‹©äº†åˆé€‚çš„é…ç½®æ–‡ä»¶

---

## ğŸ‰ æ€»ç»“

ä½ ç°åœ¨æœ‰äº†ï¼š

1. âœ… **facebook/vggt**: ä» HuggingFace åŠ è½½çš„åŸå§‹ VGGT
2. âœ… **å•å¸§è¾“å…¥**: VGGTAdapter ä¸“é—¨å¤„ç†å•å¸§å›¾åƒ
3. âœ… **Qwen3-0.6B-Base**: æœ€æ–°çš„ Qwen3 è¯­è¨€æ¨¡å‹

**å¼€å§‹è®­ç»ƒ**:
```bash
cd /workspace/tingting/AtlasVLA/vggt_vla
python scripts/test_vggt_qwen3.py  # å…ˆæµ‹è¯•
bash scripts/quick_start.sh configs/train_vggt_qwen3.yaml  # ç„¶åè®­ç»ƒ
```

ç¥è®­ç»ƒé¡ºåˆ©ï¼ğŸš€

---

**æœ€åæ›´æ–°**: 2024-02-11  
**ç‰ˆæœ¬**: 1.0
