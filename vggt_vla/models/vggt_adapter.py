"""
VGGT Adapter - åŠ è½½å®˜æ–¹ facebook/VGGT-1B æƒé‡å¹¶é€‚é…åˆ° VLA ä»»åŠ¡
ä¸“é—¨å¤„ç†å•å¸§è¾“å…¥ + è¯­è¨€æŒ‡ä»¤

å®˜æ–¹æƒé‡é€šè¿‡ model.pt ä¸‹è½½ï¼ˆä¸ vggt demo ä¸€è‡´ï¼‰ï¼Œé Transformers AutoModelã€‚
"""
import os
import sys
import torch
import torch.nn as nn
from typing import Dict, Tuple, Optional

# å®˜æ–¹ VGGT-1B æƒé‡ URLï¼ˆä¸ vggt ä»“åº“ demo ä¸€è‡´ï¼‰
VGGT_1B_WEIGHTS_URL = "https://huggingface.co/facebook/VGGT-1B/resolve/main/model.pt"


def _get_vggt_module():
    """å¯¼å…¥æœ¬åœ° vggt åŒ…ä¸­çš„ VGGT ç±»"""
    vggt_path = os.path.join(os.path.dirname(__file__), '../../vggt')
    if vggt_path not in sys.path:
        sys.path.insert(0, vggt_path)
    from vggt.models.vggt import VGGT
    return VGGT


class VGGTAdapter(nn.Module):
    """
    é€‚é… facebook/VGGT-1B åˆ° VLA ä»»åŠ¡:
    1. âœ… å¤„ç†å•å¸§è¾“å…¥ (åŸå§‹VGGTè®¾è®¡ç”¨äºè§†é¢‘åºåˆ—ï¼Œæˆ‘ä»¬é€‚é…ä¸ºå•å¸§)
    2. âœ… æ³¨å…¥ language tokens (é€šè¿‡ç‰¹æ®Šçš„èåˆæœºåˆ¶)
    3. âœ… æå–é€‚åˆ action prediction çš„ç‰¹å¾

    å®˜æ–¹æƒé‡åŠ è½½æ–¹å¼ï¼šæœ¬åœ° VGGT ç»“æ„ + ä¸‹è½½ model.pt çš„ state_dictï¼ˆä¸å®˜æ–¹ demo ä¸€è‡´ï¼‰ã€‚
    """
    def __init__(self, config):
        super().__init__()
        self.config = config

        print("\n" + "="*60)
        print("Loading official facebook/VGGT-1B weights...")
        print("="*60)

        VGGT = _get_vggt_module()

        # å®˜æ–¹ VGGT-1B ç»“æ„ï¼ˆä¸å®˜æ–¹ repo ä¸€è‡´ï¼Œæ‰èƒ½æ­£ç¡®åŠ è½½ state_dictï¼‰
        self.vggt = VGGT(
            img_size=518,
            patch_size=14,
            embed_dim=1024,
            enable_camera=True,
            enable_point=True,
            enable_depth=True,
            enable_track=True,
        )
        self.use_pretrained_vggt = False

        try:
            state_dict = torch.hub.load_state_dict_from_url(
                VGGT_1B_WEIGHTS_URL,
                map_location="cpu",
                progress=True,
            )
            # è‹¥è¿”å›çš„æ˜¯åŒ…è£… dictï¼ˆå¦‚ {'model': state_dict}ï¼‰ï¼Œåˆ™å–å‡º
            if isinstance(state_dict, dict) and "model" in state_dict and len(state_dict) == 1:
                state_dict = state_dict["model"]
            self.vggt.load_state_dict(state_dict, strict=True)
            self.use_pretrained_vggt = True
            print("âœ“ Loaded official facebook/VGGT-1B weights from HuggingFace (model.pt)")
        except Exception as e:
            print(f"âš  Could not load official weights: {e}")
            print("  Using VGGT structure with random initialization (no pretrained weights).")

        # æ£€æŸ¥ aggregatorï¼ˆæœ¬ adapter åªä½¿ç”¨è¿™éƒ¨åˆ†ï¼‰
        if hasattr(self.vggt, 'aggregator'):
            agg = self.vggt.aggregator
            has_frame = hasattr(agg, 'frame_blocks')
            has_global = hasattr(agg, 'global_blocks')
            print(f"  - Aggregator: âœ“ (frame_blocks={len(agg.frame_blocks) if has_frame else 0}, global_blocks={len(agg.global_blocks) if has_global else 0})")
        else:
            print("  - Warning: No aggregator found.")
        
        # VGGTçš„embeddingç»´åº¦
        self.vggt_embed_dim = 1024  # facebook/VGGT-1B é»˜è®¤
        self.target_dim = config.embed_dim  # æˆ‘ä»¬çš„ç›®æ ‡ç»´åº¦ (768)
        
        print(f"  VGGT embedding dim: {self.vggt_embed_dim}")
        print(f"  Target dim: {self.target_dim}")
        
        # Language token æŠ•å½±å±‚: target_dim -> vggt_embed_dim
        # Qwen3-0.6Bè¾“å‡ºéœ€è¦æŠ•å½±åˆ°VGGTç©ºé—´
        self.lang_adapter = nn.Sequential(
            nn.Linear(self.target_dim, self.vggt_embed_dim),
            nn.LayerNorm(self.vggt_embed_dim),
            nn.GELU(),
            nn.Dropout(0.1)
        )
        print(f"  Language adapter: {self.target_dim} -> {self.vggt_embed_dim}")
        
        # Vision token æŠ•å½±å±‚ (å¦‚æœç»´åº¦ä¸åŒ¹é…)
        if self.target_dim != self.vggt_embed_dim:
            self.vision_adapter = nn.Sequential(
                nn.Linear(self.target_dim, self.vggt_embed_dim),
                nn.LayerNorm(self.vggt_embed_dim)
            )
            print(f"  Vision adapter: {self.target_dim} -> {self.vggt_embed_dim}")
        else:
            self.vision_adapter = nn.Identity()
            print("  Vision adapter: Identity (dims match)")
        
        # Cross-modal attention: language attends to vision
        self.cross_attn = nn.MultiheadAttention(
            embed_dim=self.vggt_embed_dim,
            num_heads=8,
            dropout=0.1,
            batch_first=True
        )
        self.cross_attn_norm = nn.LayerNorm(self.vggt_embed_dim)
        
        # ç‰¹å¾æå–å±‚ï¼šä» VGGT è¾“å‡ºæå– VLA ç‰¹å¾
        # VGGT aggregator è¾“å‡ºæ˜¯ list of [B, S, P, 2C]ï¼Œæˆ‘ä»¬å–æœ€åä¸€å±‚
        self.feature_projector = nn.Sequential(
            nn.Linear(self.vggt_embed_dim * 2, self.vggt_embed_dim),
            nn.LayerNorm(self.vggt_embed_dim),
            nn.GELU(),
            nn.Dropout(0.1),
            nn.Linear(self.vggt_embed_dim, self.target_dim),
            nn.LayerNorm(self.target_dim)
        )
        print(f"  Feature projector: {self.vggt_embed_dim * 2} -> {self.target_dim}")
        
        # Action queries (å¯å­¦ä¹ çš„query tokensç”¨äºaction prediction)
        self.num_action_queries = 16
        self.action_queries = nn.Parameter(
            torch.randn(1, self.num_action_queries, self.target_dim)
        )
        nn.init.trunc_normal_(self.action_queries, std=0.02)
        print(f"  Action queries: {self.num_action_queries} learnable tokens")
        
        # å†»ç»“VGGT backbone (å¯é€‰)
        if config.freeze_vggt:
            print("\n  ğŸ”’ Freezing VGGT backbone...")
            for param in self.vggt.parameters():
                param.requires_grad = False
            trainable = sum(p.numel() for p in self.parameters() if p.requires_grad)
            print(f"  âœ“ Trainable parameters: {trainable / 1e6:.2f}M (adapter layers only)")
        else:
            trainable = sum(p.numel() for p in self.parameters() if p.requires_grad)
            print(f"  Trainable parameters: {trainable / 1e6:.2f}M (full model)")
        
        print("="*60 + "\n")
    
    def forward(
        self,
        vision_tokens: torch.Tensor,      # [B, N_v, D]
        language_tokens: torch.Tensor,    # [B, N_l, D]
        vision_info: Dict,
        language_info: Dict,
        language_mask: torch.Tensor
    ) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor, Dict]:
        """
        å¤„ç†å•å¸§è¾“å…¥çš„forward pass
        
        Args:
            vision_tokens: [B, N_v, D] - æ¥è‡ªvision encoderçš„tokens (å•å¸§)
            language_tokens: [B, N_l, D] - æ¥è‡ªQwen3çš„language tokens
            
        Returns:
            vision_features: [B, N_v, D]
            language_features: [B, N_l, D]
            global_features: [B, num_queries, D]
            output_info: Dict
            
        æµç¨‹:
        1. å°†å•å¸§vision tokensé€‚é…åˆ°VGGTç©ºé—´
        2. Language tokensé€šè¿‡cross-attentionä¸visionäº¤äº’
        3. ä½¿ç”¨VGGTå¤„ç†fused features
        4. æå–action-relevant features
        """
        B = vision_tokens.size(0)
        N_v = vision_tokens.size(1)
        N_l = language_tokens.size(1)
        device = vision_tokens.device
        
        # ========== Step 1: é€‚é…åˆ°VGGTç©ºé—´ ==========
        vision_adapted = self.vision_adapter(vision_tokens)  # [B, N_v, 1024]
        language_adapted = self.lang_adapter(language_tokens)  # [B, N_l, 1024]
        
        # ========== Step 2: Cross-modal interaction ==========
        # Language attends to vision (language-conditioned visual features)
        language_enhanced, _ = self.cross_attn(
            query=language_adapted,
            key=vision_adapted,
            value=vision_adapted,
            need_weights=False
        )
        language_enhanced = self.cross_attn_norm(language_enhanced + language_adapted)
        
        # ========== Step 3: ä½¿ç”¨VGGT aggregatorå¤„ç†tokens ==========
        # VGGT aggregatoræœŸæœ›tokenså½¢çŠ¶:
        # - Frame attention: [B*S, P, C] å…¶ä¸­ P æ˜¯æ¯ä¸ªframeçš„tokenæ•°
        # - Global attention: [B, S*P, C]
        # å¯¹äºå•å¸§è¾“å…¥ï¼ŒS=1
        
        try:
            aggregator = self.vggt.aggregator if hasattr(self.vggt, 'aggregator') else None
            
            if aggregator is not None and hasattr(aggregator, 'frame_blocks') and hasattr(aggregator, 'global_blocks'):
                # åˆå¹¶visionå’Œlanguage tokens
                # å½¢çŠ¶: [B, N_v+N_l, C] = [B, P, C] å…¶ä¸­ P = N_v + N_l
                combined_tokens = torch.cat([vision_adapted, language_enhanced], dim=1)  # [B, P, C]
                
                B = combined_tokens.size(0)
                S = 1  # å•å¸§è¾“å…¥
                P = combined_tokens.size(1)  # æ€»tokenæ•° (N_v + N_l)
                C = combined_tokens.size(2)  # embed_dim (1024)
                
                # æ£€æŸ¥ç»´åº¦æ˜¯å¦åŒ¹é…
                if C != self.vggt_embed_dim:
                    raise ValueError(f"Token dimension {C} does not match VGGT embed_dim {self.vggt_embed_dim}")
                
                # æ„é€ ä½ç½®ç¼–ç  (å¦‚æœéœ€è¦)
                pos = None
                if hasattr(aggregator, 'position_getter') and aggregator.position_getter is not None:
                    # ä»vision_infoè·å–gridä¿¡æ¯æ¥æ„é€ ä½ç½®ç¼–ç 
                    grid_size = vision_info.get('grid_size', int(N_v ** 0.5))
                    # ä¸ºvision tokensæ„é€ 2Dä½ç½®ç¼–ç 
                    # ä¸ºlanguage tokensæ·»åŠ è™šæ‹Ÿä½ç½®ï¼ˆä½¿ç”¨vision tokensçš„æœ€åä¸€ä¸ªä½ç½®ï¼‰
                    if grid_size > 0 and N_v > 0:
                        try:
                            # æ„é€ vision tokensçš„2Dä½ç½®
                            vision_pos = aggregator.position_getter(
                                B * S, grid_size, grid_size, device=combined_tokens.device
                            )  # [B*S, grid_size*grid_size, 2]
                            
                            # å¦‚æœvision_posçš„tokenæ•°å°‘äºN_vï¼Œéœ€è¦æ‰©å±•æˆ–æˆªæ–­
                            if vision_pos.size(1) < N_v:
                                # æ‰©å±•ï¼šé‡å¤æœ€åä¸€ä¸ªä½ç½®
                                last_pos = vision_pos[:, -1:, :]
                                padding = last_pos.expand(-1, N_v - vision_pos.size(1), -1)
                                vision_pos = torch.cat([vision_pos, padding], dim=1)
                            elif vision_pos.size(1) > N_v:
                                # æˆªæ–­ï¼šåªå–å‰N_vä¸ª
                                vision_pos = vision_pos[:, :N_v, :]
                            
                            # ä¸ºlanguage tokensæ·»åŠ ä½ç½®ï¼ˆä½¿ç”¨æœ€åä¸€ä¸ªvision tokençš„ä½ç½®ï¼‰
                            if N_l > 0:
                                lang_pos = vision_pos[:, -1:, :].expand(-1, N_l, -1)  # [B*S, N_l, 2]
                                # åˆå¹¶ä½ç½®ç¼–ç 
                                pos = torch.cat([vision_pos, lang_pos], dim=1)  # [B*S, P, 2]
                            else:
                                pos = vision_pos  # [B*S, N_v, 2]
                        except Exception as e:
                            # å¦‚æœä½ç½®ç¼–ç æ„é€ å¤±è´¥ï¼Œä½¿ç”¨None
                            pos = None
                
                # ä½¿ç”¨VGGTçš„alternating attentionæœºåˆ¶
                # å‚è€ƒ aggregator._process_frame_attention å’Œ _process_global_attention
                frame_idx = 0
                global_idx = 0
                frame_intermediates = []
                global_intermediates = []
                
                # è·å–alternating attentionçš„é…ç½®
                aa_order = aggregator.aa_order if hasattr(aggregator, 'aa_order') else ["frame", "global"]
                aa_block_num = aggregator.aa_block_num if hasattr(aggregator, 'aa_block_num') else len(aggregator.frame_blocks)
                aa_block_size = aggregator.aa_block_size if hasattr(aggregator, 'aa_block_size') else 1
                
                tokens = combined_tokens
                
                for _ in range(aa_block_num):
                    for attn_type in aa_order:
                        if attn_type == "frame":
                            # Frame attention: [B*S, P, C]
                            if tokens.shape != (B * S, P, C):
                                tokens = tokens.view(B, S, P, C).view(B * S, P, C)
                            
                            if pos is not None and pos.shape != (B * S, P, 2):
                                pos_frame = pos.view(B, S, P, 2).view(B * S, P, 2)
                            else:
                                pos_frame = pos
                            
                            # å¤„ç†frame blocks
                            for _ in range(aa_block_size):
                                if frame_idx < len(aggregator.frame_blocks):
                                    if self.training:
                                        tokens = torch.utils.checkpoint.checkpoint(
                                            aggregator.frame_blocks[frame_idx], 
                                            tokens, pos_frame, 
                                            use_reentrant=aggregator.use_reentrant if hasattr(aggregator, 'use_reentrant') else False
                                        )
                                    else:
                                        tokens = aggregator.frame_blocks[frame_idx](tokens, pos=pos_frame)
                                    frame_idx += 1
                                    frame_intermediates.append(tokens.view(B, S, P, C))
                        
                        elif attn_type == "global":
                            # Global attention: [B, S*P, C]
                            if tokens.shape != (B, S * P, C):
                                tokens = tokens.view(B, S, P, C).view(B, S * P, C)
                            
                            if pos is not None and pos.shape != (B, S * P, 2):
                                pos_global = pos.view(B, S, P, 2).view(B, S * P, 2)
                            else:
                                pos_global = pos
                            
                            # å¤„ç†global blocks
                            for _ in range(aa_block_size):
                                if global_idx < len(aggregator.global_blocks):
                                    if self.training:
                                        tokens = torch.utils.checkpoint.checkpoint(
                                            aggregator.global_blocks[global_idx],
                                            tokens, pos_global,
                                            use_reentrant=aggregator.use_reentrant if hasattr(aggregator, 'use_reentrant') else False
                                        )
                                    else:
                                        tokens = aggregator.global_blocks[global_idx](tokens, pos=pos_global)
                                    global_idx += 1
                                    global_intermediates.append(tokens.view(B, S, P, C))
                
                # åˆå¹¶frameå’Œglobalçš„ä¸­é—´ç‰¹å¾ï¼ˆç±»ä¼¼aggregatorçš„è¾“å‡ºï¼‰
                # å–æœ€åä¸€å±‚çš„è¾“å‡º
                if len(frame_intermediates) > 0 and len(global_intermediates) > 0:
                    # å¯¹é½é•¿åº¦
                    min_len = min(len(frame_intermediates), len(global_intermediates))
                    frame_final = frame_intermediates[-1]  # [B, S, P, C]
                    global_final = global_intermediates[-1]  # [B, S, P, C]
                    
                    # Concat frameå’Œglobalç‰¹å¾: [B, S, P, 2C]
                    concat_features = torch.cat([frame_final, global_final], dim=-1)  # [B, 1, P, 2C]
                    concat_features = concat_features.squeeze(1)  # [B, P, 2C]
                    
                    # æŠ•å½±å›ç›®æ ‡ç»´åº¦
                    x_projected = self.feature_projector(concat_features)  # [B, P, D]
                else:
                    # Fallback: å¦‚æœæ²¡æœ‰ä¸­é—´ç‰¹å¾ï¼Œç›´æ¥ä½¿ç”¨æœ€åçš„tokens
                    tokens_final = tokens.view(B, S, P, C).squeeze(1)  # [B, P, C]
                    x_projected = self.feature_projector(torch.cat([tokens_final, tokens_final], dim=-1))
                
            else:
                # Fallback: aggregatorä¸å¯ç”¨
                raise AttributeError("VGGT aggregator does not have required attributes")
                
        except Exception as e:
            import traceback
            # åªåœ¨ç¬¬ä¸€æ¬¡é”™è¯¯æ—¶æ‰“å°è¯¦ç»†tracebackï¼Œé¿å…æ—¥å¿—è¿‡å¤š
            if not hasattr(self, '_vggt_error_logged'):
                error_msg = f"Error in VGGT processing: {e}\n{traceback.format_exc()}"
                print(f"Warning: {error_msg}")
                print("Using simple concatenation fallback")
                self._vggt_error_logged = True
            else:
                # åç»­é”™è¯¯åªæ‰“å°ç®€çŸ­ä¿¡æ¯
                print(f"Warning: VGGT processing error (using fallback): {type(e).__name__}")
            
            x = torch.cat([vision_adapted, language_enhanced], dim=1)
            x_projected = self.feature_projector(torch.cat([x, x], dim=-1))
        
        # ========== Step 4: åˆ†ç¦»features ==========
        vision_features = x_projected[:, :N_v, :]       # [B, N_v, D]
        language_features = x_projected[:, N_v:, :]     # [B, N_l, D]
        
        # ========== Step 5: ç”Ÿæˆglobal features for action ==========
        # ä½¿ç”¨å¯å­¦ä¹ çš„action queries
        global_features = self.action_queries.expand(B, -1, -1)  # [B, num_queries, D]
        
        # Attention-based feature aggregation
        # Action queries attend to all features
        all_features = x_projected  # [B, N_v+N_l, D]
        pooled = all_features.mean(dim=1, keepdim=True).expand(-1, self.num_action_queries, -1)
        global_features = global_features + 0.1 * pooled  # Weighted combination
        
        output_info = {
            'vggt_embed_dim': self.vggt_embed_dim,
            'target_dim': self.target_dim,
            'num_vision_tokens': N_v,
            'num_language_tokens': N_l,
            'num_action_queries': self.num_action_queries,
            'single_frame_input': True  # æ ‡è®°è¿™æ˜¯å•å¸§è¾“å…¥
        }
        
        return vision_features, language_features, global_features, output_info


class SimpleVGGTBackbone(nn.Module):
    """
    ç®€åŒ–ç‰ˆVGGT Backbone - ä¸ä¾èµ–HuggingFace
    ç”¨äºå¿«é€Ÿå®éªŒå’Œè°ƒè¯•
    """
    def __init__(self, config):
        super().__init__()
        self.config = config
        
        from .components.token_fusion import TokenFusion
        from .components.graph_builder import GraphBuilder
        from .components.vggt_layers import VGGTLayer
        
        self.token_fusion = TokenFusion(config)
        self.graph_builder = GraphBuilder(config)
        
        self.layers = nn.ModuleList([
            VGGTLayer(config)
            for _ in range(config.depth)
        ])
        
        self.norm = nn.LayerNorm(config.embed_dim)
        
        # Action queries
        self.num_action_queries = 16
        self.action_queries = nn.Parameter(
            torch.randn(1, self.num_action_queries, config.embed_dim)
        )
        nn.init.trunc_normal_(self.action_queries, std=0.02)
    
    def forward(
        self,
        vision_tokens: torch.Tensor,
        language_tokens: torch.Tensor,
        vision_info: Dict,
        language_info: Dict,
        language_mask: torch.Tensor
    ) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor, Dict]:
        
        B = vision_tokens.size(0)
        device = vision_tokens.device
        
        # Token Fusion
        fused_tokens, attention_mask, fusion_info = self.token_fusion(
            vision_tokens, language_tokens,
            vision_info, language_info, language_mask
        )
        
        # Build Graph
        edge_index, edge_attr = self.graph_builder.build_graph(
            fusion_info, batch_size=B, device=device
        )
        
        # VGGT Layers
        x = fused_tokens
        for layer in self.layers:
            x = layer(x, edge_index, attn_mask=attention_mask)
        
        x = self.norm(x)
        
        # Split features
        lang_start, lang_end = fusion_info['language_token_range']
        vis_start, vis_end = fusion_info['vision_token_range']
        
        language_features = x[:, lang_start:lang_end, :]
        vision_features = x[:, vis_start:vis_end, :]
        
        # Global features
        global_features = self.action_queries.expand(B, -1, -1)
        
        output_info = fusion_info.copy()
        output_info['num_action_queries'] = self.num_action_queries
        
        return vision_features, language_features, global_features, output_info
