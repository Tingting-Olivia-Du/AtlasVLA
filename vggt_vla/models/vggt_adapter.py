"""
VGGT Adapter - Âä†ËΩΩÂÆòÊñπ facebook/VGGT-1B ÊùÉÈáçÂπ∂ÈÄÇÈÖçÂà∞ VLA ‰ªªÂä°
‰∏ìÈó®Â§ÑÁêÜÂçïÂ∏ßËæìÂÖ• + ËØ≠Ë®ÄÊåá‰ª§

ÂÆòÊñπÊùÉÈáçÈÄöËøá model.pt ‰∏ãËΩΩÔºà‰∏é vggt demo ‰∏ÄËá¥ÔºâÔºåÈùû Transformers AutoModel„ÄÇ
"""
import os
import sys
import torch
import torch.nn as nn
from typing import Dict, Tuple, Optional

# ÂÆòÊñπ VGGT-1B ÊùÉÈáç URLÔºà‰∏é vggt ‰ªìÂ∫ì demo ‰∏ÄËá¥Ôºâ
VGGT_1B_WEIGHTS_URL = "https://huggingface.co/facebook/VGGT-1B/resolve/main/model.pt"


def _get_vggt_module():
    """ÂØºÂÖ•Êú¨Âú∞ vggt ÂåÖ‰∏≠ÁöÑ VGGT Á±ª"""
    vggt_path = os.path.join(os.path.dirname(__file__), '../../vggt')
    if vggt_path not in sys.path:
        sys.path.insert(0, vggt_path)
    from vggt.models.vggt import VGGT
    return VGGT


class VGGTAdapter(nn.Module):
    """
    ÈÄÇÈÖç facebook/VGGT-1B Âà∞ VLA ‰ªªÂä°:
    1. ‚úÖ Â§ÑÁêÜÂçïÂ∏ßËæìÂÖ• (ÂéüÂßãVGGTËÆæËÆ°Áî®‰∫éËßÜÈ¢ëÂ∫èÂàóÔºåÊàë‰ª¨ÈÄÇÈÖç‰∏∫ÂçïÂ∏ß) //‰ΩÜÊòØlibero Êúâ‰∏§‰∏™ËßÜËßíÁöÑÁÖßÁâá
    2. ‚úÖ Ê≥®ÂÖ• language tokens (ÈÄöËøáÁâπÊÆäÁöÑËûçÂêàÊú∫Âà∂)
    3. ‚úÖ ÊèêÂèñÈÄÇÂêà action prediction ÁöÑÁâπÂæÅ

    ÂÆòÊñπÊùÉÈáçÂä†ËΩΩÊñπÂºèÔºöÊú¨Âú∞ VGGT ÁªìÊûÑ + ‰∏ãËΩΩ model.pt ÁöÑ state_dictÔºà‰∏éÂÆòÊñπ demo ‰∏ÄËá¥Ôºâ„ÄÇ
    """
    def __init__(self, config):
        super().__init__()
        self.config = config

        print("\n" + "="*60)
        print("Loading official facebook/VGGT-1B weights...")
        print("="*60)

        VGGT = _get_vggt_module()

        # ÂÆòÊñπ VGGT-1B ÁªìÊûÑÔºà‰∏éÂÆòÊñπ repo ‰∏ÄËá¥ÔºåÊâçËÉΩÊ≠£Á°ÆÂä†ËΩΩ state_dictÔºâ
        self.vggt = VGGT(
            img_size=518,
            patch_size=14,
            embed_dim=1024,
            enable_camera=True,
            enable_point=True,
            enable_depth=True,
            enable_track=True,
        )
        self.use_pretrained_vggt = False

        try:
            state_dict = torch.hub.load_state_dict_from_url(
                VGGT_1B_WEIGHTS_URL,
                map_location="cpu",
                progress=True,
            )
            # Ëã•ËøîÂõûÁöÑÊòØÂåÖË£Ö dictÔºàÂ¶Ç {'model': state_dict}ÔºâÔºåÂàôÂèñÂá∫
            if isinstance(state_dict, dict) and "model" in state_dict and len(state_dict) == 1:
                state_dict = state_dict["model"]
            self.vggt.load_state_dict(state_dict, strict=True)
            self.use_pretrained_vggt = True
            print("‚úì Loaded official facebook/VGGT-1B weights from HuggingFace (model.pt)")
        except Exception as e:
            print(f"‚ö† Could not load official weights: {e}")
            print("  Using VGGT structure with random initialization (no pretrained weights).")

        # Ê£ÄÊü• aggregatorÔºàÊú¨ adapter Âè™‰ΩøÁî®ËøôÈÉ®ÂàÜÔºâ
        if hasattr(self.vggt, 'aggregator'):
            agg = self.vggt.aggregator
            has_frame = hasattr(agg, 'frame_blocks')
            has_global = hasattr(agg, 'global_blocks')
            print(f"  - Aggregator: ‚úì (frame_blocks={len(agg.frame_blocks) if has_frame else 0}, global_blocks={len(agg.global_blocks) if has_global else 0})")
        else:
            print("  - Warning: No aggregator found.")
        
        # VGGTÁöÑembeddingÁª¥Â∫¶
        self.vggt_embed_dim = 1024  # facebook/VGGT-1B ÈªòËÆ§
        self.target_dim = config.embed_dim  # Êàë‰ª¨ÁöÑÁõÆÊ†áÁª¥Â∫¶ (768)
        
        print(f"  VGGT embedding dim: {self.vggt_embed_dim}")
        print(f"  Target dim: {self.target_dim}")
        
        # Language token ÊäïÂΩ±Â±Ç: target_dim -> vggt_embed_dim
        # Qwen3-0.6BËæìÂá∫ÈúÄË¶ÅÊäïÂΩ±Âà∞VGGTÁ©∫Èó¥
        self.lang_adapter = nn.Sequential(
            nn.Linear(self.target_dim, self.vggt_embed_dim),
            nn.LayerNorm(self.vggt_embed_dim),
            nn.GELU(),
            nn.Dropout(0.1)
        )
        print(f"  Language adapter: {self.target_dim} -> {self.vggt_embed_dim}")
        
        # Vision token ÊäïÂΩ±Â±Ç (Â¶ÇÊûúÁª¥Â∫¶‰∏çÂåπÈÖç)
        if self.target_dim != self.vggt_embed_dim:
            self.vision_adapter = nn.Sequential(
                nn.Linear(self.target_dim, self.vggt_embed_dim),
                nn.LayerNorm(self.vggt_embed_dim)
            )
            print(f"  Vision adapter: {self.target_dim} -> {self.vggt_embed_dim}")
        else:
            self.vision_adapter = nn.Identity()
            print("  Vision adapter: Identity (dims match)")
        
        # Cross-modal attention: language attends to vision
        self.cross_attn = nn.MultiheadAttention(
            embed_dim=self.vggt_embed_dim,
            num_heads=8,
            dropout=0.1,
            batch_first=True
        )
        self.cross_attn_norm = nn.LayerNorm(self.vggt_embed_dim)
        
        # ÁâπÂæÅÊèêÂèñÂ±ÇÔºö‰ªé VGGT ËæìÂá∫ÊèêÂèñ VLA ÁâπÂæÅ
        # VGGT aggregator ËæìÂá∫ÊòØ list of [B, S, P, 2C]ÔºåÊàë‰ª¨ÂèñÊúÄÂêé‰∏ÄÂ±Ç
        self.feature_projector = nn.Sequential(
            nn.Linear(self.vggt_embed_dim * 2, self.vggt_embed_dim),
            nn.LayerNorm(self.vggt_embed_dim),
            nn.GELU(),
            nn.Dropout(0.1),
            nn.Linear(self.vggt_embed_dim, self.target_dim),
            nn.LayerNorm(self.target_dim)
        )
        print(f"  Feature projector: {self.vggt_embed_dim * 2} -> {self.target_dim}")
        
        # Action queries (ÂèØÂ≠¶‰π†ÁöÑquery tokensÁî®‰∫éaction prediction)
        self.num_action_queries = 16
        self.action_queries = nn.Parameter(
            torch.randn(1, self.num_action_queries, self.target_dim)
        )
        nn.init.trunc_normal_(self.action_queries, std=0.02)
        print(f"  Action queries: {self.num_action_queries} learnable tokens")
        
        # ÂÜªÁªìVGGT backbone (ÂèØÈÄâ)
        if config.freeze_vggt:
            print("\n  üîí Freezing VGGT backbone...")
            for param in self.vggt.parameters():
                param.requires_grad = False
            trainable = sum(p.numel() for p in self.parameters() if p.requires_grad)
            print(f"  ‚úì Trainable parameters: {trainable / 1e6:.2f}M (adapter layers only)")
        else:
            trainable = sum(p.numel() for p in self.parameters() if p.requires_grad)
            print(f"  Trainable parameters: {trainable / 1e6:.2f}M (full model)")
        
        print("="*60 + "\n")
    
    def forward(
        self,
        vision_tokens: torch.Tensor,      # [B, N_v, D]
        language_tokens: torch.Tensor,    # [B, N_l, D]
        vision_info: Dict,
        language_info: Dict,
        language_mask: torch.Tensor
    ) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor, Dict]:
        """
        Â§ÑÁêÜÂçïÂ∏ßËæìÂÖ•ÁöÑforward pass
        
        Args:
            vision_tokens: [B, N_v, D] - Êù•Ëá™vision encoderÁöÑtokens (ÂçïÂ∏ß)
            language_tokens: [B, N_l, D] - Êù•Ëá™Qwen3ÁöÑlanguage tokens
            
        Returns:
            vision_features: [B, N_v, D]
            language_features: [B, N_l, D]
            global_features: [B, num_queries, D]
            output_info: Dict
            
        ÊµÅÁ®ã:
        1. Â∞ÜÂçïÂ∏ßvision tokensÈÄÇÈÖçÂà∞VGGTÁ©∫Èó¥
        2. Language tokensÈÄöËøácross-attention‰∏évision‰∫§‰∫í
        3. ‰ΩøÁî®VGGTÂ§ÑÁêÜfused features
        4. ÊèêÂèñaction-relevant features
        """
        B = vision_tokens.size(0)
        N_v = vision_tokens.size(1)
        N_l = language_tokens.size(1)
        device = vision_tokens.device
        
        # ========== Step 1: ÈÄÇÈÖçÂà∞VGGTÁ©∫Èó¥ ==========
        vision_adapted = self.vision_adapter(vision_tokens)  # [B, N_v, 1024]
        language_adapted = self.lang_adapter(language_tokens)  # [B, N_l, 1024]
        
        # ========== Step 2: Cross-modal interaction ==========
        # Language attends to vision (language-conditioned visual features)
        language_enhanced, _ = self.cross_attn(
            query=language_adapted,
            key=vision_adapted,
            value=vision_adapted,
            need_weights=False
        )
        language_enhanced = self.cross_attn_norm(language_enhanced + language_adapted)
        
        # ========== Step 3: ‰ΩøÁî®VGGT aggregatorÂ§ÑÁêÜtokens ==========
        # VGGT aggregatorÊúüÊúõtokensÂΩ¢Áä∂:
        # - Frame attention: [B*S, P, C] ÂÖ∂‰∏≠ P ÊòØÊØè‰∏™frameÁöÑtokenÊï∞
        # - Global attention: [B, S*P, C]
        # ÂØπ‰∫éÂçïÂ∏ßËæìÂÖ•ÔºåS=1
        
        try:
            aggregator = self.vggt.aggregator if hasattr(self.vggt, 'aggregator') else None
            
            if aggregator is not None and hasattr(aggregator, 'frame_blocks') and hasattr(aggregator, 'global_blocks'):
                # ÂêàÂπ∂visionÂíålanguage tokens
                # ÂΩ¢Áä∂: [B, N_v+N_l, C] = [B, P, C] ÂÖ∂‰∏≠ P = N_v + N_l
                combined_tokens = torch.cat([vision_adapted, language_enhanced], dim=1)  # [B, P, C]
                
                B = combined_tokens.size(0)
                S = 1  # ÂçïÂ∏ßËæìÂÖ•
                P = combined_tokens.size(1)  # ÊÄªtokenÊï∞ (N_v + N_l)
                C = combined_tokens.size(2)  # embed_dim (1024)
                
                # Ê£ÄÊü•Áª¥Â∫¶ÊòØÂê¶ÂåπÈÖç
                if C != self.vggt_embed_dim:
                    raise ValueError(f"Token dimension {C} does not match VGGT embed_dim {self.vggt_embed_dim}")
                
                # ÊûÑÈÄ†‰ΩçÁΩÆÁºñÁ†Å (Â¶ÇÊûúÈúÄË¶Å)
                pos = None
                if hasattr(aggregator, 'position_getter') and aggregator.position_getter is not None and N_v > 0:
                    try:
                        # Â§öËßÜËßíÊó∂ vision_info Âê´ patch_positions [2*P, 2]ÔºåÁõ¥Êé•‰ΩøÁî®
                        if vision_info.get("patch_positions") is not None:
                            pp = vision_info["patch_positions"]
                            vision_pos = pp.unsqueeze(0).expand(B * S, -1, -1).to(
                                device=combined_tokens.device, dtype=combined_tokens.dtype
                            )
                        else:
                            grid_size = vision_info.get('grid_size', int(N_v ** 0.5))
                            vision_pos = aggregator.position_getter(
                                B * S, grid_size, grid_size, device=combined_tokens.device
                            )
                            if vision_pos.size(1) < N_v:
                                last_pos = vision_pos[:, -1:, :]
                                vision_pos = torch.cat([
                                    vision_pos,
                                    last_pos.expand(-1, N_v - vision_pos.size(1), -1)
                                ], dim=1)
                            elif vision_pos.size(1) > N_v:
                                vision_pos = vision_pos[:, :N_v, :]
                        if N_l > 0:
                            lang_pos = vision_pos[:, -1:, :].expand(-1, N_l, -1)
                            pos = torch.cat([vision_pos, lang_pos], dim=1)
                        else:
                            pos = vision_pos
                    except Exception as e:
                        pos = None
                
                # ‰ΩøÁî®VGGTÁöÑalternating attentionÊú∫Âà∂
                # ÂèÇËÄÉ aggregator._process_frame_attention Âíå _process_global_attention
                frame_idx = 0
                global_idx = 0
                frame_intermediates = []
                global_intermediates = []
                
                # Ëé∑Âèñalternating attentionÁöÑÈÖçÁΩÆ
                aa_order = aggregator.aa_order if hasattr(aggregator, 'aa_order') else ["frame", "global"]
                aa_block_num = aggregator.aa_block_num if hasattr(aggregator, 'aa_block_num') else len(aggregator.frame_blocks)
                aa_block_size = aggregator.aa_block_size if hasattr(aggregator, 'aa_block_size') else 1
                
                tokens = combined_tokens
                
                for _ in range(aa_block_num):
                    for attn_type in aa_order:
                        if attn_type == "frame":
                            # Frame attention: [B*S, P, C]
                            if tokens.shape != (B * S, P, C):
                                tokens = tokens.view(B, S, P, C).view(B * S, P, C)
                            
                            if pos is not None and pos.shape != (B * S, P, 2):
                                pos_frame = pos.view(B, S, P, 2).view(B * S, P, 2)
                            else:
                                pos_frame = pos
                            
                            # Â§ÑÁêÜframe blocks
                            for _ in range(aa_block_size):
                                if frame_idx < len(aggregator.frame_blocks):
                                    if self.training:
                                        tokens = torch.utils.checkpoint.checkpoint(
                                            aggregator.frame_blocks[frame_idx], 
                                            tokens, pos_frame, 
                                            use_reentrant=aggregator.use_reentrant if hasattr(aggregator, 'use_reentrant') else False
                                        )
                                    else:
                                        tokens = aggregator.frame_blocks[frame_idx](tokens, pos=pos_frame)
                                    frame_idx += 1
                                    frame_intermediates.append(tokens.view(B, S, P, C))
                        
                        elif attn_type == "global":
                            # Global attention: [B, S*P, C]
                            if tokens.shape != (B, S * P, C):
                                tokens = tokens.view(B, S, P, C).view(B, S * P, C)
                            
                            if pos is not None and pos.shape != (B, S * P, 2):
                                pos_global = pos.view(B, S, P, 2).view(B, S * P, 2)
                            else:
                                pos_global = pos
                            
                            # Â§ÑÁêÜglobal blocks
                            for _ in range(aa_block_size):
                                if global_idx < len(aggregator.global_blocks):
                                    if self.training:
                                        tokens = torch.utils.checkpoint.checkpoint(
                                            aggregator.global_blocks[global_idx],
                                            tokens, pos_global,
                                            use_reentrant=aggregator.use_reentrant if hasattr(aggregator, 'use_reentrant') else False
                                        )
                                    else:
                                        tokens = aggregator.global_blocks[global_idx](tokens, pos=pos_global)
                                    global_idx += 1
                                    global_intermediates.append(tokens.view(B, S, P, C))
                
                # ÂêàÂπ∂frameÂíåglobalÁöÑ‰∏≠Èó¥ÁâπÂæÅÔºàÁ±ª‰ººaggregatorÁöÑËæìÂá∫Ôºâ
                # ÂèñÊúÄÂêé‰∏ÄÂ±ÇÁöÑËæìÂá∫
                if len(frame_intermediates) > 0 and len(global_intermediates) > 0:
                    # ÂØπÈΩêÈïøÂ∫¶
                    min_len = min(len(frame_intermediates), len(global_intermediates))
                    frame_final = frame_intermediates[-1]  # [B, S, P, C]
                    global_final = global_intermediates[-1]  # [B, S, P, C]
                    
                    # Concat frameÂíåglobalÁâπÂæÅ: [B, S, P, 2C]
                    concat_features = torch.cat([frame_final, global_final], dim=-1)  # [B, 1, P, 2C]
                    concat_features = concat_features.squeeze(1)  # [B, P, 2C]
                    
                    # ÊäïÂΩ±ÂõûÁõÆÊ†áÁª¥Â∫¶
                    x_projected = self.feature_projector(concat_features)  # [B, P, D]
                else:
                    # Fallback: Â¶ÇÊûúÊ≤°Êúâ‰∏≠Èó¥ÁâπÂæÅÔºåÁõ¥Êé•‰ΩøÁî®ÊúÄÂêéÁöÑtokens
                    tokens_final = tokens.view(B, S, P, C).squeeze(1)  # [B, P, C]
                    x_projected = self.feature_projector(torch.cat([tokens_final, tokens_final], dim=-1))
                
            else:
                # Fallback: aggregator‰∏çÂèØÁî®
                raise AttributeError("VGGT aggregator does not have required attributes")
                
        except Exception as e:
            import traceback
            # Âè™Âú®Á¨¨‰∏ÄÊ¨°ÈîôËØØÊó∂ÊâìÂç∞ËØ¶ÁªÜtracebackÔºåÈÅøÂÖçÊó•ÂøóËøáÂ§ö
            if not hasattr(self, '_vggt_error_logged'):
                error_msg = f"Error in VGGT processing: {e}\n{traceback.format_exc()}"
                print(f"Warning: {error_msg}")
                print("Using simple concatenation fallback")
                self._vggt_error_logged = True
            else:
                # ÂêéÁª≠ÈîôËØØÂè™ÊâìÂç∞ÁÆÄÁü≠‰ø°ÊÅØ
                print(f"Warning: VGGT processing error (using fallback): {type(e).__name__}")
            
            x = torch.cat([vision_adapted, language_enhanced], dim=1)
            x_projected = self.feature_projector(torch.cat([x, x], dim=-1))
        
        # ========== Step 4: ÂàÜÁ¶ªfeatures ==========
        vision_features = x_projected[:, :N_v, :]       # [B, N_v, D]
        language_features = x_projected[:, N_v:, :]     # [B, N_l, D]
        
        # ========== Step 5: ÁîüÊàêglobal features for action ==========
        # ‰ΩøÁî®ÂèØÂ≠¶‰π†ÁöÑaction queries
        global_features = self.action_queries.expand(B, -1, -1)  # [B, num_queries, D]
        
        # Attention-based feature aggregation
        # Action queries attend to all features
        all_features = x_projected  # [B, N_v+N_l, D]
        pooled = all_features.mean(dim=1, keepdim=True).expand(-1, self.num_action_queries, -1)
        global_features = global_features + 0.1 * pooled  # Weighted combination
        
        output_info = {
            'vggt_embed_dim': self.vggt_embed_dim,
            'target_dim': self.target_dim,
            'num_vision_tokens': N_v,
            'num_language_tokens': N_l,
            'num_action_queries': self.num_action_queries,
            'single_frame_input': True  # Ê†áËÆ∞ËøôÊòØÂçïÂ∏ßËæìÂÖ•
        }
        
        return vision_features, language_features, global_features, output_info


class SimpleVGGTBackbone(nn.Module):
    """
    ÁÆÄÂåñÁâàVGGT Backbone - ‰∏ç‰æùËµñHuggingFace
    Áî®‰∫éÂø´ÈÄüÂÆûÈ™åÂíåË∞ÉËØï
    """
    def __init__(self, config):
        super().__init__()
        self.config = config
        
        from .components.token_fusion import TokenFusion
        from .components.graph_builder import GraphBuilder
        from .components.vggt_layers import VGGTLayer
        
        self.token_fusion = TokenFusion(config)
        self.graph_builder = GraphBuilder(config)
        
        self.layers = nn.ModuleList([
            VGGTLayer(config)
            for _ in range(config.depth)
        ])
        
        self.norm = nn.LayerNorm(config.embed_dim)
        
        # Action queries
        self.num_action_queries = 16
        self.action_queries = nn.Parameter(
            torch.randn(1, self.num_action_queries, config.embed_dim)
        )
        nn.init.trunc_normal_(self.action_queries, std=0.02)
    
    def forward(
        self,
        vision_tokens: torch.Tensor,
        language_tokens: torch.Tensor,
        vision_info: Dict,
        language_info: Dict,
        language_mask: torch.Tensor
    ) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor, Dict]:
        
        B = vision_tokens.size(0)
        device = vision_tokens.device
        
        # Token Fusion
        fused_tokens, attention_mask, fusion_info = self.token_fusion(
            vision_tokens, language_tokens,
            vision_info, language_info, language_mask
        )
        
        # Build Graph
        edge_index, edge_attr = self.graph_builder.build_graph(
            fusion_info, batch_size=B, device=device
        )
        
        # VGGT Layers
        x = fused_tokens
        for layer in self.layers:
            x = layer(x, edge_index, attn_mask=attention_mask)
        
        x = self.norm(x)
        
        # Split features
        lang_start, lang_end = fusion_info['language_token_range']
        vis_start, vis_end = fusion_info['vision_token_range']
        
        language_features = x[:, lang_start:lang_end, :]
        vision_features = x[:, vis_start:vis_end, :]
        
        # Global features
        global_features = self.action_queries.expand(B, -1, -1)
        
        output_info = fusion_info.copy()
        output_info['num_action_queries'] = self.num_action_queries
        
        return vision_features, language_features, global_features, output_info
