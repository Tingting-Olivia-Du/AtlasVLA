"""
VGGT Adapter - ä»Ž HuggingFace åŠ è½½ facebook/vggt å¹¶é€‚é…åˆ° VLA ä»»åŠ¡
ä¸“é—¨å¤„ç†å•å¸§è¾“å…¥ + è¯­è¨€æŒ‡ä»¤
"""
import torch
import torch.nn as nn
from typing import Dict, Tuple, Optional
from transformers import AutoModel


class VGGTAdapter(nn.Module):
    """
    é€‚é… facebook/vggt åˆ° VLA ä»»åŠ¡:
    1. âœ… å¤„ç†å•å¸§è¾“å…¥ (åŽŸå§‹VGGTè®¾è®¡ç”¨äºŽè§†é¢‘åºåˆ—ï¼Œæˆ‘ä»¬é€‚é…ä¸ºå•å¸§)
    2. âœ… æ³¨å…¥ language tokens (é€šè¿‡ç‰¹æ®Šçš„èžåˆæœºåˆ¶)
    3. âœ… æå–é€‚åˆ action prediction çš„ç‰¹å¾
    
    å…³é”®æ”¹è¿›:
    - å•å¸§å›¾åƒè¢«æ‰©å±•ä¸ºä¼ªè§†é¢‘åºåˆ— [B, 1, 3, H, W] ä»¥é€‚é…VGGT
    - Language tokensé€šè¿‡attentionæœºåˆ¶ä¸Žvisual featuresäº¤äº’
    - ä½¿ç”¨learnable action queriesæå–ä»»åŠ¡ç›¸å…³ç‰¹å¾
    """
    def __init__(self, config):
        super().__init__()
        self.config = config
        
        # åŠ è½½é¢„è®­ç»ƒçš„ VGGT
        print("\n" + "="*60)
        print("Loading facebook/vggt from HuggingFace...")
        print("="*60)
        try:
            self.vggt = AutoModel.from_pretrained(
                "facebook/vggt",
                trust_remote_code=True
            )
            print("âœ“ Successfully loaded facebook/vggt from HuggingFace")
            self.use_pretrained_vggt = True
        except Exception as e:
            print(f"âš  Warning: Could not load facebook/vggt from HuggingFace: {e}")
            print("Falling back to local VGGT implementation...")
            try:
                # Fallback: ä»Žæœ¬åœ°vggtç›®å½•åŠ è½½
                import sys
                import os
                vggt_path = os.path.join(os.path.dirname(__file__), '../../vggt')
                if vggt_path not in sys.path:
                    sys.path.insert(0, vggt_path)
                
                from vggt.models.vggt import VGGT
                self.vggt = VGGT(
                    img_size=224,  # é€‚é…æˆ‘ä»¬çš„è¾“å…¥å°ºå¯¸
                    patch_size=16,
                    embed_dim=1024,
                    enable_camera=False,
                    enable_point=False,
                    enable_depth=False,
                    enable_track=False
                )
                print("âœ“ Successfully loaded VGGT from local implementation")
                self.use_pretrained_vggt = False
            except Exception as e2:
                print(f"âœ— Error loading local VGGT: {e2}")
                raise RuntimeError("Cannot load VGGT. Please install vggt or check HuggingFace access.")
        
        # VGGTçš„embeddingç»´åº¦
        self.vggt_embed_dim = 1024  # facebook/vggt é»˜è®¤
        self.target_dim = config.embed_dim  # æˆ‘ä»¬çš„ç›®æ ‡ç»´åº¦ (768)
        
        print(f"  VGGT embedding dim: {self.vggt_embed_dim}")
        print(f"  Target dim: {self.target_dim}")
        
        # Language token æŠ•å½±å±‚: target_dim -> vggt_embed_dim
        # Qwen3-0.6Bè¾“å‡ºéœ€è¦æŠ•å½±åˆ°VGGTç©ºé—´
        self.lang_adapter = nn.Sequential(
            nn.Linear(self.target_dim, self.vggt_embed_dim),
            nn.LayerNorm(self.vggt_embed_dim),
            nn.GELU(),
            nn.Dropout(0.1)
        )
        print(f"  Language adapter: {self.target_dim} -> {self.vggt_embed_dim}")
        
        # Vision token æŠ•å½±å±‚ (å¦‚æžœç»´åº¦ä¸åŒ¹é…)
        if self.target_dim != self.vggt_embed_dim:
            self.vision_adapter = nn.Sequential(
                nn.Linear(self.target_dim, self.vggt_embed_dim),
                nn.LayerNorm(self.vggt_embed_dim)
            )
            print(f"  Vision adapter: {self.target_dim} -> {self.vggt_embed_dim}")
        else:
            self.vision_adapter = nn.Identity()
            print("  Vision adapter: Identity (dims match)")
        
        # Cross-modal attention: language attends to vision
        self.cross_attn = nn.MultiheadAttention(
            embed_dim=self.vggt_embed_dim,
            num_heads=8,
            dropout=0.1,
            batch_first=True
        )
        self.cross_attn_norm = nn.LayerNorm(self.vggt_embed_dim)
        
        # ç‰¹å¾æå–å±‚ï¼šä»Ž VGGT è¾“å‡ºæå– VLA ç‰¹å¾
        # VGGT aggregator è¾“å‡ºæ˜¯ list of [B, S, P, 2C]ï¼Œæˆ‘ä»¬å–æœ€åŽä¸€å±‚
        self.feature_projector = nn.Sequential(
            nn.Linear(self.vggt_embed_dim * 2, self.vggt_embed_dim),
            nn.LayerNorm(self.vggt_embed_dim),
            nn.GELU(),
            nn.Dropout(0.1),
            nn.Linear(self.vggt_embed_dim, self.target_dim),
            nn.LayerNorm(self.target_dim)
        )
        print(f"  Feature projector: {self.vggt_embed_dim * 2} -> {self.target_dim}")
        
        # Action queries (å¯å­¦ä¹ çš„query tokensç”¨äºŽaction prediction)
        self.num_action_queries = 16
        self.action_queries = nn.Parameter(
            torch.randn(1, self.num_action_queries, self.target_dim)
        )
        nn.init.trunc_normal_(self.action_queries, std=0.02)
        print(f"  Action queries: {self.num_action_queries} learnable tokens")
        
        # å†»ç»“VGGT backbone (å¯é€‰)
        if config.freeze_vggt:
            print("\n  ðŸ”’ Freezing VGGT backbone...")
            for param in self.vggt.parameters():
                param.requires_grad = False
            trainable = sum(p.numel() for p in self.parameters() if p.requires_grad)
            print(f"  âœ“ Trainable parameters: {trainable / 1e6:.2f}M (adapter layers only)")
        else:
            trainable = sum(p.numel() for p in self.parameters() if p.requires_grad)
            print(f"  Trainable parameters: {trainable / 1e6:.2f}M (full model)")
        
        print("="*60 + "\n")
    
    def forward(
        self,
        vision_tokens: torch.Tensor,      # [B, N_v, D]
        language_tokens: torch.Tensor,    # [B, N_l, D]
        vision_info: Dict,
        language_info: Dict,
        language_mask: torch.Tensor
    ) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor, Dict]:
        """
        å¤„ç†å•å¸§è¾“å…¥çš„forward pass
        
        Args:
            vision_tokens: [B, N_v, D] - æ¥è‡ªvision encoderçš„tokens (å•å¸§)
            language_tokens: [B, N_l, D] - æ¥è‡ªQwen3çš„language tokens
            
        Returns:
            vision_features: [B, N_v, D]
            language_features: [B, N_l, D]
            global_features: [B, num_queries, D]
            output_info: Dict
            
        æµç¨‹:
        1. å°†å•å¸§vision tokensé€‚é…åˆ°VGGTç©ºé—´
        2. Language tokensé€šè¿‡cross-attentionä¸Žvisionäº¤äº’
        3. ä½¿ç”¨VGGTå¤„ç†fused features
        4. æå–action-relevant features
        """
        B = vision_tokens.size(0)
        N_v = vision_tokens.size(1)
        N_l = language_tokens.size(1)
        device = vision_tokens.device
        
        # ========== Step 1: é€‚é…åˆ°VGGTç©ºé—´ ==========
        vision_adapted = self.vision_adapter(vision_tokens)  # [B, N_v, 1024]
        language_adapted = self.lang_adapter(language_tokens)  # [B, N_l, 1024]
        
        # ========== Step 2: Cross-modal interaction ==========
        # Language attends to vision (language-conditioned visual features)
        language_enhanced, _ = self.cross_attn(
            query=language_adapted,
            key=vision_adapted,
            value=vision_adapted,
            need_weights=False
        )
        language_enhanced = self.cross_attn_norm(language_enhanced + language_adapted)
        
        # ========== Step 3: å‡†å¤‡VGGTè¾“å…¥ (å•å¸§) ==========
        # å°†vision tokensé‡å¡‘ä¸ºVGGTæœŸæœ›çš„æ ¼å¼
        # VGGTæœŸæœ›: [B, S, 3, H, W] å…¶ä¸­ S æ˜¯åºåˆ—é•¿åº¦
        # å¯¹äºŽå•å¸§ï¼Œæˆ‘ä»¬è®¾ç½® S=1
        
        # ä½†æ˜¯æˆ‘ä»¬å·²ç»æœ‰tokensäº†ï¼Œéœ€è¦æž„é€ ä¼ªå›¾åƒæˆ–ç›´æŽ¥ä½¿ç”¨aggregator
        # è¿™é‡Œé‡‡ç”¨ç›´æŽ¥ä½¿ç”¨aggregatorçš„æ–¹æ¡ˆ
        
        try:
            # å°è¯•ä½¿ç”¨VGGTçš„aggregator
            aggregator = self.vggt.aggregator if hasattr(self.vggt, 'aggregator') else None
            
            if aggregator is not None and hasattr(aggregator, 'frame_blocks'):
                # ä½¿ç”¨VGGTçš„transformer blocks
                # å°†visionå’Œlanguage tokensä½œä¸ºè¾“å…¥
                x = torch.cat([vision_adapted, language_enhanced], dim=1)  # [B, N_v+N_l, 1024]
                
                # VGGT alternating attention
                num_layers = min(len(aggregator.frame_blocks), len(aggregator.global_blocks))
                for i in range(num_layers):
                    # Frame-level attention
                    if hasattr(aggregator, 'frame_blocks'):
                        x = aggregator.frame_blocks[i](x, pos=None)
                    # Global attention
                    if hasattr(aggregator, 'global_blocks'):
                        x = aggregator.global_blocks[i](x, pos=None)
                
                # æŠ•å½±å›žç›®æ ‡ç»´åº¦
                # VGGTè¾“å‡ºéœ€è¦concat (æ¨¡æ‹Ÿframeå’Œglobalç‰¹å¾)
                x_projected = self.feature_projector(torch.cat([x, x], dim=-1))  # [B, N_v+N_l, D]
                
            else:
                # Fallback: ç®€å•çš„transformerå¤„ç†
                print("Warning: Using fallback path (VGGT aggregator not available)")
                x = torch.cat([vision_adapted, language_enhanced], dim=1)
                # ç®€å•æŠ•å½±
                x_projected = self.feature_projector(torch.cat([x, x], dim=-1))
                
        except Exception as e:
            print(f"Warning: Error in VGGT processing: {e}")
            print("Using simple concatenation fallback")
            x = torch.cat([vision_adapted, language_enhanced], dim=1)
            x_projected = self.feature_projector(torch.cat([x, x], dim=-1))
        
        # ========== Step 4: åˆ†ç¦»features ==========
        vision_features = x_projected[:, :N_v, :]       # [B, N_v, D]
        language_features = x_projected[:, N_v:, :]     # [B, N_l, D]
        
        # ========== Step 5: ç”Ÿæˆglobal features for action ==========
        # ä½¿ç”¨å¯å­¦ä¹ çš„action queries
        global_features = self.action_queries.expand(B, -1, -1)  # [B, num_queries, D]
        
        # Attention-based feature aggregation
        # Action queries attend to all features
        all_features = x_projected  # [B, N_v+N_l, D]
        pooled = all_features.mean(dim=1, keepdim=True).expand(-1, self.num_action_queries, -1)
        global_features = global_features + 0.1 * pooled  # Weighted combination
        
        output_info = {
            'vggt_embed_dim': self.vggt_embed_dim,
            'target_dim': self.target_dim,
            'num_vision_tokens': N_v,
            'num_language_tokens': N_l,
            'num_action_queries': self.num_action_queries,
            'single_frame_input': True  # æ ‡è®°è¿™æ˜¯å•å¸§è¾“å…¥
        }
        
        return vision_features, language_features, global_features, output_info


class SimpleVGGTBackbone(nn.Module):
    """
    ç®€åŒ–ç‰ˆVGGT Backbone - ä¸ä¾èµ–HuggingFace
    ç”¨äºŽå¿«é€Ÿå®žéªŒå’Œè°ƒè¯•
    """
    def __init__(self, config):
        super().__init__()
        self.config = config
        
        from .components.token_fusion import TokenFusion
        from .components.graph_builder import GraphBuilder
        from .components.vggt_layers import VGGTLayer
        
        self.token_fusion = TokenFusion(config)
        self.graph_builder = GraphBuilder(config)
        
        self.layers = nn.ModuleList([
            VGGTLayer(config)
            for _ in range(config.depth)
        ])
        
        self.norm = nn.LayerNorm(config.embed_dim)
        
        # Action queries
        self.num_action_queries = 16
        self.action_queries = nn.Parameter(
            torch.randn(1, self.num_action_queries, config.embed_dim)
        )
        nn.init.trunc_normal_(self.action_queries, std=0.02)
    
    def forward(
        self,
        vision_tokens: torch.Tensor,
        language_tokens: torch.Tensor,
        vision_info: Dict,
        language_info: Dict,
        language_mask: torch.Tensor
    ) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor, Dict]:
        
        B = vision_tokens.size(0)
        device = vision_tokens.device
        
        # Token Fusion
        fused_tokens, attention_mask, fusion_info = self.token_fusion(
            vision_tokens, language_tokens,
            vision_info, language_info, language_mask
        )
        
        # Build Graph
        edge_index, edge_attr = self.graph_builder.build_graph(
            fusion_info, batch_size=B, device=device
        )
        
        # VGGT Layers
        x = fused_tokens
        for layer in self.layers:
            x = layer(x, edge_index, attn_mask=attention_mask)
        
        x = self.norm(x)
        
        # Split features
        lang_start, lang_end = fusion_info['language_token_range']
        vis_start, vis_end = fusion_info['vision_token_range']
        
        language_features = x[:, lang_start:lang_end, :]
        vision_features = x[:, vis_start:vis_end, :]
        
        # Global features
        global_features = self.action_queries.expand(B, -1, -1)
        
        output_info = fusion_info.copy()
        output_info['num_action_queries'] = self.num_action_queries
        
        return vision_features, language_features, global_features, output_info
